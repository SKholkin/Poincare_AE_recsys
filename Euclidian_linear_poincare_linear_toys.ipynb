{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7d529b6",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting git+https://github.com/SaitejaUtpala/rieoptax.git\n",
      "  Cloning https://github.com/SaitejaUtpala/rieoptax.git to /tmp/pip-req-build-8ylw217z\n",
      "  Running command git clone --filter=blob:none --quiet https://github.com/SaitejaUtpala/rieoptax.git /tmp/pip-req-build-8ylw217z\n",
      "  Resolved https://github.com/SaitejaUtpala/rieoptax.git to commit 1954a45e9e2b8e14307fefd1cd2ae494402cf4ab\n",
      "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hRequirement already satisfied: absl-py in /home/skholkin/projects/python_venv/lib/python3.10/site-packages (from rieoptax==0.0.1) (1.4.0)\n",
      "Collecting autodp\n",
      "  Using cached autodp-0.2.tar.gz (39 kB)\n",
      "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hRequirement already satisfied: chex in /home/skholkin/projects/python_venv/lib/python3.10/site-packages (from rieoptax==0.0.1) (0.1.6)\n",
      "Requirement already satisfied: flax in /home/skholkin/projects/python_venv/lib/python3.10/site-packages (from rieoptax==0.0.1) (0.6.7)\n",
      "Requirement already satisfied: jax in /home/skholkin/projects/python_venv/lib/python3.10/site-packages (from rieoptax==0.0.1) (0.4.6)\n",
      "Requirement already satisfied: jaxlib in /home/skholkin/projects/python_venv/lib/python3.10/site-packages (from rieoptax==0.0.1) (0.4.6)\n",
      "Requirement already satisfied: numpy in /home/skholkin/projects/python_venv/lib/python3.10/site-packages (from rieoptax==0.0.1) (1.23.5)\n",
      "Requirement already satisfied: typing_extensions in /home/skholkin/projects/python_venv/lib/python3.10/site-packages (from rieoptax==0.0.1) (4.4.0)\n",
      "Requirement already satisfied: scipy in /home/skholkin/projects/python_venv/lib/python3.10/site-packages (from autodp->rieoptax==0.0.1) (1.10.0)\n",
      "Requirement already satisfied: dm-tree>=0.1.5 in /home/skholkin/projects/python_venv/lib/python3.10/site-packages (from chex->rieoptax==0.0.1) (0.1.8)\n",
      "Requirement already satisfied: toolz>=0.9.0 in /home/skholkin/projects/python_venv/lib/python3.10/site-packages (from chex->rieoptax==0.0.1) (0.12.0)\n",
      "Requirement already satisfied: opt-einsum in /home/skholkin/projects/python_venv/lib/python3.10/site-packages (from jax->rieoptax==0.0.1) (3.3.0)\n",
      "Requirement already satisfied: msgpack in /home/skholkin/projects/python_venv/lib/python3.10/site-packages (from flax->rieoptax==0.0.1) (1.0.4)\n",
      "Requirement already satisfied: tensorstore in /home/skholkin/projects/python_venv/lib/python3.10/site-packages (from flax->rieoptax==0.0.1) (0.1.33)\n",
      "Requirement already satisfied: rich>=11.1 in /home/skholkin/projects/python_venv/lib/python3.10/site-packages (from flax->rieoptax==0.0.1) (13.3.2)\n",
      "Requirement already satisfied: orbax in /home/skholkin/projects/python_venv/lib/python3.10/site-packages (from flax->rieoptax==0.0.1) (0.1.5)\n",
      "Requirement already satisfied: PyYAML>=5.4.1 in /home/skholkin/projects/python_venv/lib/python3.10/site-packages (from flax->rieoptax==0.0.1) (6.0)\n",
      "Requirement already satisfied: optax in /home/skholkin/projects/python_venv/lib/python3.10/site-packages (from flax->rieoptax==0.0.1) (0.1.4)\n",
      "Requirement already satisfied: markdown-it-py<3.0.0,>=2.2.0 in /home/skholkin/projects/python_venv/lib/python3.10/site-packages (from rich>=11.1->flax->rieoptax==0.0.1) (2.2.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /home/skholkin/projects/python_venv/lib/python3.10/site-packages (from rich>=11.1->flax->rieoptax==0.0.1) (2.14.0)\n",
      "Requirement already satisfied: importlib_resources in /home/skholkin/projects/python_venv/lib/python3.10/site-packages (from orbax->flax->rieoptax==0.0.1) (5.12.0)\n",
      "Requirement already satisfied: nest_asyncio in /home/skholkin/projects/python_venv/lib/python3.10/site-packages (from orbax->flax->rieoptax==0.0.1) (1.5.6)\n",
      "Requirement already satisfied: cached_property in /home/skholkin/projects/python_venv/lib/python3.10/site-packages (from orbax->flax->rieoptax==0.0.1) (1.5.2)\n",
      "Requirement already satisfied: etils in /home/skholkin/projects/python_venv/lib/python3.10/site-packages (from orbax->flax->rieoptax==0.0.1) (1.1.0)\n",
      "Requirement already satisfied: mdurl~=0.1 in /home/skholkin/projects/python_venv/lib/python3.10/site-packages (from markdown-it-py<3.0.0,>=2.2.0->rich>=11.1->flax->rieoptax==0.0.1) (0.1.2)\n",
      "Building wheels for collected packages: rieoptax, autodp\n",
      "  Building wheel for rieoptax (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for rieoptax: filename=rieoptax-0.0.1-py3-none-any.whl size=29035 sha256=9c70712c794ed64879206bec61df9f91f4cd71e6a53f49fb821a4c8e8de5d89d\n",
      "  Stored in directory: /tmp/pip-ephem-wheel-cache-rxxamq01/wheels/a7/9f/69/52ec93d81804da7aae50235c8309ff8415b9cfbf071e4d4350\n",
      "  Building wheel for autodp (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for autodp: filename=autodp-0.2-py3-none-any.whl size=42443 sha256=576df3ffc2dbc47fde9e05ccdc1c9f6fce022101e0d3209cd5485e6c87ba5878\n",
      "  Stored in directory: /home/skholkin/.cache/pip/wheels/4e/58/65/b22c7a93f9650f8c7ce140b5309a5a0563c7119ad6261b3b93\n",
      "Successfully built rieoptax autodp\n",
      "Installing collected packages: autodp, rieoptax\n",
      "Successfully installed autodp-0.2 rieoptax-0.0.1\n"
     ]
    }
   ],
   "source": [
    "!pip install git+https://github.com/SaitejaUtpala/rieoptax.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c6720ecb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/skholkin/projects/python_venv/lib/python3.10/site-packages/flax/core/frozen_dict.py:169: FutureWarning: jax.tree_util.register_keypaths is deprecated, and will be removed in a future release. Please use `register_pytree_with_keys()` instead.\n",
      "  jax.tree_util.register_keypaths(\n",
      "No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)\n"
     ]
    }
   ],
   "source": [
    "import jax\n",
    "import jax.numpy as jnp\n",
    "from flax import linen as nn\n",
    "from flax import struct\n",
    "\n",
    "import optax\n",
    "\n",
    "import numpy as np\n",
    "from flax import struct \n",
    "from clu import metrics\n",
    "from dataclasses import field\n",
    "from functools import partial\n",
    "from typing import Any, Callable, Optional, Tuple, Union\n",
    "from flax import struct  \n",
    "\n",
    "from clu import metrics\n",
    "\n",
    "from chex import Array\n",
    "from flax import linen as nn\n",
    "from flax.linen.activation import sigmoid, tanh\n",
    "from flax.linen.dtypes import promote_dtype\n",
    "from flax.linen.initializers import orthogonal\n",
    "from flax.linen.linear import default_kernel_init\n",
    "from jax import numpy as jnp\n",
    "from jax import random, vmap\n",
    "from chex import Array\n",
    "from jax import lax\n",
    "from jax.nn.initializers import Initializer as Initializer\n",
    "from jax._src import dtypes\n",
    "\n",
    "from flax.training import train_state\n",
    "\n",
    "from rieoptax.geometry.hyperbolic import PoincareBall\n",
    "key = jax.random.PRNGKey(0)\n",
    "\n",
    "\n",
    "@struct.dataclass\n",
    "class Metrics(metrics.Collection):\n",
    "  loss: metrics.Average.from_output('loss')\n",
    "    \n",
    "class TrainState(train_state.TrainState):\n",
    "  metrics: Metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9205f88f",
   "metadata": {},
   "source": [
    "## Poincare ball manifold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ff6e45a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from rieoptax.geometry.hyperbolic import PoincareBall\n",
    "dims, c = 10, -1\n",
    "ball = PoincareBall(dims, c)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "332144d0",
   "metadata": {},
   "source": [
    "### Linear layer FLAX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8371c425",
   "metadata": {},
   "outputs": [],
   "source": [
    "dim = 10\n",
    "key = jax.random.PRNGKey(0)\n",
    "\n",
    "\n",
    "class RealFnEuclidian:\n",
    "    def __init__(self, in_dim=10, out_dim=5):\n",
    "        global key\n",
    "        self.theta_vec = jax.random.normal(key, shape=(out_dim, in_dim))\n",
    "        key, subkey = jax.random.split(key)\n",
    "        self.theta_bias = jax.random.normal(subkey, shape=(out_dim,))\n",
    "        \n",
    "    def apply_fn(self, x):\n",
    "        return self.theta_vec @ x + self.theta_bias\n",
    "    \n",
    "    \n",
    "real_fn = RealFnEuclidian(in_dim=10, out_dim=5)\n",
    "\n",
    "key, subkey = jax.random.split(key)\n",
    "sampled_x = jax.random.normal(subkey, shape=(10,))\n",
    "sampled_y = real_fn.apply_fn(sampled_x)\n",
    "\n",
    "\n",
    "def sample_gen(dim=10): \n",
    "    global key\n",
    "    key, subkey = jax.random.split(key)\n",
    "    sampled_x = jax.random.normal(key, shape=(dim,))\n",
    "    return sampled_x, real_fn.apply_fn(sampled_x) \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "90dd3edb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(Array([-0.08943313,  1.5241055 , -0.41166604,  0.8208107 ,  1.0238154 ,\n",
       "         0.4196539 ,  1.0200214 ,  0.00804093,  0.63066274,  1.7859513 ],      dtype=float32),\n",
       " Array([-2.1657333, -4.6946745, -3.1985538,  3.8316498, -4.455423 ],      dtype=float32),\n",
       " Array([[-1.9496337e-01, -1.2271470e+00, -9.1779011e-01,  1.4520884e-01,\n",
       "          1.8150680e-03, -7.4958706e-01, -8.0263335e-01,  1.2242906e-01,\n",
       "         -1.7589016e-01,  1.0580742e+00],\n",
       "        [-2.4492617e-01, -7.7644986e-01,  4.0735185e-01,  5.3473938e-01,\n",
       "         -1.3593255e+00, -2.1800435e+00, -2.7196446e-01,  1.3657150e+00,\n",
       "          3.0998212e-01,  3.4722403e-01],\n",
       "        [-3.6875853e-01,  2.7483419e-01,  1.2167790e+00, -2.9819357e-01,\n",
       "         -1.7567271e+00,  1.5572840e+00, -5.2775639e-01,  1.2997842e-01,\n",
       "         -1.2879779e+00, -1.3760062e+00],\n",
       "        [ 5.8405155e-01,  4.5868692e-01,  4.1681662e-02, -5.9727866e-02,\n",
       "          3.0754104e-01,  1.6455851e-01, -5.2474406e-02,  1.1677581e+00,\n",
       "          1.7214864e+00,  3.6793044e-01],\n",
       "        [-9.4078797e-01, -1.1206886e-01,  7.2438514e-01,  9.1082263e-01,\n",
       "         -9.7785103e-01, -3.1999704e-01, -1.4206324e+00, -1.0327141e-01,\n",
       "         -8.7865436e-01, -3.9354938e-01]], dtype=float32),\n",
       " Array([-1.4581939, -2.047044 ,  2.0473392,  1.1684095, -0.9758364],      dtype=float32))"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sampled_x, sampled_y, real_fn.theta_vec, real_fn.theta_bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fd135e40",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]]\n",
      "\n",
      "\u001b[3m                               Linear Summary                                \u001b[0m\n",
      "┏━━━━━━━━━┳━━━━━━━━┳━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
      "┃\u001b[1m \u001b[0m\u001b[1mpath   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mmodule\u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1minputs       \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1moutputs     \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mparams                 \u001b[0m\u001b[1m \u001b[0m┃\n",
      "┡━━━━━━━━━╇━━━━━━━━╇━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
      "│         │ Linear │ \u001b[2mfloat32\u001b[0m[1,10] │ \u001b[2mfloat32\u001b[0m[1,5] │ Dense_0:                │\n",
      "│         │        │               │              │   bias: \u001b[2mfloat32\u001b[0m[5]      │\n",
      "│         │        │               │              │   kernel: \u001b[2mfloat32\u001b[0m[10,5] │\n",
      "│         │        │               │              │                         │\n",
      "│         │        │               │              │ \u001b[1m55 \u001b[0m\u001b[1;2m(220 B)\u001b[0m              │\n",
      "├─────────┼────────┼───────────────┼──────────────┼─────────────────────────┤\n",
      "│ Dense_0 │ Dense  │ \u001b[2mfloat32\u001b[0m[1,10] │ \u001b[2mfloat32\u001b[0m[1,5] │ bias: \u001b[2mfloat32\u001b[0m[5]        │\n",
      "│         │        │               │              │ kernel: \u001b[2mfloat32\u001b[0m[10,5]   │\n",
      "│         │        │               │              │                         │\n",
      "│         │        │               │              │ \u001b[1m55 \u001b[0m\u001b[1;2m(220 B)\u001b[0m              │\n",
      "├─────────┼────────┼───────────────┼──────────────┼─────────────────────────┤\n",
      "│\u001b[1m \u001b[0m\u001b[1m       \u001b[0m\u001b[1m \u001b[0m│\u001b[1m \u001b[0m\u001b[1m      \u001b[0m\u001b[1m \u001b[0m│\u001b[1m \u001b[0m\u001b[1m             \u001b[0m\u001b[1m \u001b[0m│\u001b[1m \u001b[0m\u001b[1m       Total\u001b[0m\u001b[1m \u001b[0m│\u001b[1m \u001b[0m\u001b[1m110 \u001b[0m\u001b[1;2m(440 B)\u001b[0m\u001b[1m            \u001b[0m\u001b[1m \u001b[0m│\n",
      "└─────────┴────────┴───────────────┴──────────────┴─────────────────────────┘\n",
      "\u001b[1m                                                                             \u001b[0m\n",
      "\u001b[1m                        Total Parameters: 110 \u001b[0m\u001b[1;2m(440 B)\u001b[0m\u001b[1m                        \u001b[0m\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "class Linear(nn.Module):\n",
    "  @nn.compact\n",
    "  def __call__(self, x):\n",
    "    print(x)\n",
    "    y = nn.Dense(5)(x)\n",
    "    return y\n",
    "\n",
    "linear = Linear()\n",
    "print(linear.tabulate(jax.random.PRNGKey(42), jnp.ones((1, 10))))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ecbfa597",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]]\n"
     ]
    }
   ],
   "source": [
    "batch_size = 1\n",
    "\n",
    "@struct.dataclass\n",
    "class Metrics(metrics.Collection):\n",
    "  loss: metrics.Average.from_output('loss')\n",
    "    \n",
    "from flax.training import train_state\n",
    "class TrainState(train_state.TrainState):\n",
    "  metrics: Metrics\n",
    "\n",
    "def create_train_state(module, rng, learning_rate):\n",
    "  \"\"\"Creates an initial `TrainState`.\"\"\"\n",
    "  params = module.init(rng, jnp.ones([batch_size, 10]))['params'] # initialize parameters by passing a template image\n",
    "  tx = optax.sgd(learning_rate)\n",
    "  return TrainState.create(\n",
    "      apply_fn=module.apply, params=params, tx=tx,\n",
    "      metrics=Metrics.empty())\n",
    "\n",
    "train_state = create_train_state(linear, jax.random.PRNGKey(43), 1e-2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "585363cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Traced<ShapedArray(float32[10])>with<DynamicJaxprTrace(level=1/0)>\n",
      "Traced<ShapedArray(float32[10])>with<DynamicJaxprTrace(level=1/0)>\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Array(5.6716547, dtype=float32)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "@jax.jit\n",
    "def train_step(state, batch_x, batch_y):\n",
    "  \"\"\"Train for a single step.\"\"\"\n",
    "\n",
    "  def loss_fn(params):\n",
    "    y = state.apply_fn({'params': params}, batch_x)\n",
    "    loss = ((y - batch_y) ** 2).sum()\n",
    "    return loss\n",
    "\n",
    "  grad_fn = jax.grad(loss_fn)\n",
    "  grads = grad_fn(state.params)\n",
    "  state = state.apply_gradients(grads=grads)\n",
    "  return state\n",
    "\n",
    "@jax.jit\n",
    "def compute_metrics(*, state, batch_x, batch_y):\n",
    "    y = state.apply_fn({'params': state.params}, batch_x)\n",
    "    loss = ((y - batch_y) ** 2).mean()\n",
    "    metric_updates = state.metrics.single_from_model_output(loss=loss)\n",
    "    metrics = state.metrics.merge(metric_updates)\n",
    "    state = state.replace(metrics=metrics)\n",
    "    return state\n",
    "\n",
    "batch_x, batch_y  = sample_gen(dim=10)\n",
    "\n",
    "train_step(train_state, batch_x, batch_y)\n",
    "\n",
    "train_state = compute_metrics(state=train_state, batch_x=batch_x,batch_y=batch_y)\n",
    "train_state.metrics.compute()['loss']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "86721548",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 Loss: 7.048534393310547\n",
      "100 Loss: 1.7467708587646484\n",
      "200 Loss: 0.9494543075561523\n",
      "300 Loss: 0.6369355916976929\n",
      "400 Loss: 0.4785136878490448\n",
      "500 Loss: 0.3831923305988312\n",
      "600 Loss: 0.3195391297340393\n",
      "700 Loss: 0.2740207314491272\n",
      "800 Loss: 0.23985354602336884\n",
      "900 Loss: 0.21326224505901337\n"
     ]
    }
   ],
   "source": [
    "n_iter = 1000\n",
    "\n",
    "for i in range(n_iter):\n",
    "  # Run optimization steps over training batches and compute batch metrics\n",
    "    \n",
    "    batch_x, batch_y  = sample_gen(dim=10)\n",
    "#     print(batch_x, batch_y)\n",
    "    train_state = train_step(train_state, batch_x, batch_y)\n",
    "    train_state = compute_metrics(state=train_state, batch_x=batch_x,batch_y=batch_y)\n",
    "    loss = train_state.metrics.compute()['loss']\n",
    "    if i % 100 == 0:\n",
    "        print(f'{i} Loss: {loss}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "170d9a4d",
   "metadata": {},
   "source": [
    "### Linear layer on Poincare Ball"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "95df33f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from rieoptax.geometry.hyperbolic import PoincareBall\n",
    "\n",
    "class NormalPoincareBall:\n",
    "    def __init__(self, dim, c):\n",
    "        self.dim = dim\n",
    "        self.c = c\n",
    "        self.ball = PoincareBall(dim, c)\n",
    "        self.base_point = jnp.zeros(dim)\n",
    "    \n",
    "    def sample(self):\n",
    "        global key\n",
    "        key, subkey = jax.random.split(key)\n",
    "        euclidian_sample = jax.random.normal(key, shape=self.dim)\n",
    "        poincare_ball_sample = self.ball.exp(self.base_point, euclidian_sample)\n",
    "        return poincare_ball_sample\n",
    "\n",
    "    \n",
    "def sample_gen(dim=10): \n",
    "    global key\n",
    "    key, subkey = jax.random.split(key)\n",
    "    sampled_x = jax.random.normal(key, shape=(dim,))\n",
    "    return sampled_x, real_fn.apply_fn(sampled_x) \n",
    "\n",
    "\n",
    "poincare_ball_normal = NormalPoincareBall(dim=(10,), c=-1)\n",
    "manifold_sample = poincare_ball_normal.sample()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2d7b8858",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Array(0.99998385, dtype=float32)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "class RealFnPoincareBall:\n",
    "    def __init__(self, in_dim=10, out_dim=5, c=-1):\n",
    "        self.c = c\n",
    "        self.in_dim, self.out_dim = in_dim, out_dim\n",
    "        global key\n",
    "        key, subkey = jax.random.split(key)\n",
    "        self.theta_vec = jax.random.normal(key, shape=(out_dim, in_dim))\n",
    "        \n",
    "        poincare_ball_normal_bias = NormalPoincareBall(dim=(out_dim,), c=-1)\n",
    "        self.theta_bias = poincare_ball_normal_bias.sample()\n",
    "\n",
    "    def apply_fn(self, x):\n",
    "        ball_in = PoincareBall(self.in_dim, self.c)\n",
    "        Ax = ball_in.mobius_matvec(self.theta_vec, x)\n",
    "        ball_out = PoincareBall(self.out_dim, self.c)\n",
    "        Ax_plus_b = ball_out.mobius_add(Ax, self.theta_bias)\n",
    "        return Ax_plus_b\n",
    "\n",
    "real_fn_poincare = RealFnPoincareBall(in_dim=10, out_dim=5)\n",
    "\n",
    "poincare_ball_normal_x = NormalPoincareBall(dim=(10,), c=-1)\n",
    "\n",
    "sampled_poincare = poincare_ball_normal_x.sample()\n",
    "sampled_y = real_fn_poincare.apply_fn(sampled_poincare)\n",
    "\n",
    "jnp.linalg.norm(sampled_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "07ac1b40",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "in_dim, out_dim = 10, 5\n",
    "c = -1\n",
    "\n",
    "class PoincareLinear(nn.Module):\n",
    "    \n",
    "    param_dtype = jnp.float32\n",
    "    kernel_init: Callable = default_kernel_init\n",
    "    bias_init: Callable = nn.initializers.uniform(scale= 1 / jnp.sqrt(in_dim))\n",
    "        \n",
    "    in_dim, out_dim = in_dim, out_dim\n",
    "    c = c\n",
    "    \n",
    "    def setup(self):\n",
    "        self.scalars = self.param(\n",
    "            \"scalars\",\n",
    "            self.kernel_init,\n",
    "            (self.out_dim, self.in_dim),\n",
    "            self.param_dtype,\n",
    "        )\n",
    "    \n",
    "        self.bias_poincare = self.param(\n",
    "            \"bias_poincare\",\n",
    "            self.bias_init,\n",
    "            (self.out_dim,),\n",
    "            self.param_dtype,\n",
    "        )\n",
    "\n",
    "    def __call__(self, x):\n",
    "        ball_in = PoincareBall(self.in_dim, self.c)\n",
    "        y = ball_in.mobius_matvec(self.scalars, x)\n",
    "        \n",
    "        ball_out = PoincareBall(self.out_dim, self.c)\n",
    "\n",
    "        return ball_out.mobius_add(y, self.bias_poincare)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "f0f7cb81",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 1\n",
    "\n",
    "import flax\n",
    "import numpy as np\n",
    "import optax\n",
    "    \n",
    "from flax.training import train_state\n",
    "class TrainState(train_state.TrainState):\n",
    "    metrics: Metrics\n",
    "    \n",
    "    def apply_gradients(self, *, grads, **kwargs):\n",
    "        \"\"\"Updates `step`, `params`, `opt_state` and `**kwargs` in return value.\n",
    "\n",
    "        Note that internally this function calls `.tx.update()` followed by a call\n",
    "        to `optax.apply_updates()` to update `params` and `opt_state`.\n",
    "\n",
    "        Args:\n",
    "          grads: Gradients that have the same pytree structure as `.params`.\n",
    "          **kwargs: Additional dataclass attributes that should be `.replace()`-ed.\n",
    "\n",
    "        Returns:\n",
    "          An updated instance of `self` with `step` incremented by one, `params`\n",
    "          and `opt_state` updated by applying `grads`, and additional attributes\n",
    "          replaced as specified by `kwargs`.\n",
    "        \"\"\"\n",
    "        updates, new_opt_state = self.tx.update(\n",
    "            grads, self.opt_state, self.params)\n",
    "        \n",
    "        \n",
    "        old_bias_params = self.params['bias_poincare']\n",
    "        \n",
    "        ball_bias = PoincareBall(out_dim, c)\n",
    "        \n",
    "        bias_poincare_grads = updates['bias_poincare']\n",
    "        \n",
    "        r_grad_poincare_bias = ball_bias.egrad_to_rgrad(old_bias_params, bias_poincare_grads)\n",
    "        \n",
    "        updates = updates.unfreeze()\n",
    "        updates['bias_poincare'] = r_grad_poincare_bias\n",
    "        updates = flax.core.frozen_dict.freeze(updates)\n",
    "        \n",
    "        new_params = optax.apply_updates(self.params, updates)\n",
    "        \n",
    "        bias_poincare_new = new_params['bias_poincare']\n",
    "        new_params = new_params.unfreeze()\n",
    "        lr = self.lr\n",
    "        tv = lr * r_grad_poincare_bias\n",
    "        \n",
    "        new_params['bias_poincare'] = ball_bias.exp(old_bias_params, tv)\n",
    "        \n",
    "        new_params = flax.core.frozen_dict.freeze(new_params)\n",
    "        \n",
    "        return self.replace(\n",
    "            step=self.step + 1,\n",
    "            params=new_params,\n",
    "            opt_state=new_opt_state,\n",
    "            **kwargs,\n",
    "        )\n",
    "\n",
    "\n",
    "\n",
    "def create_train_state(module, rng, learning_rate):\n",
    "  \"\"\"Creates an initial `TrainState`.\"\"\"\n",
    "  from flax.training import train_state\n",
    "  class TrainState(train_state.TrainState):\n",
    "    metrics: Metrics\n",
    "    lr = learning_rate\n",
    "  params = module.init(rng, jnp.ones([in_dim]))['params'] # initialize parameters by passing a template image\n",
    "  tx = optax.sgd(learning_rate)\n",
    "  return TrainState.create(\n",
    "      apply_fn=module.apply, params=params, tx=tx,\n",
    "      metrics=Metrics.empty())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "5cab7355",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "@jax.jit\n",
    "def train_step(state, batch_x, batch_y):\n",
    "  \"\"\"Train for a single step.\"\"\"\n",
    "\n",
    "  def loss_fn(params):\n",
    "    y = state.apply_fn({'params': params}, batch_x)\n",
    "    loss = ((y - batch_y) ** 2).sum()\n",
    "    return loss\n",
    "  \n",
    "  grad_fn = jax.grad(loss_fn)\n",
    "  grads = grad_fn(state.params)\n",
    "  state = state.apply_gradients(grads=grads)\n",
    "  return state\n",
    "\n",
    "@jax.jit\n",
    "def compute_metrics(*, state, batch_x, batch_y):\n",
    "    y = state.apply_fn({'params': state.params}, batch_x)\n",
    "    loss = ((y - batch_y) ** 2).mean()\n",
    "    metric_updates = state.metrics.single_from_model_output(loss=loss)\n",
    "    metrics = state.metrics.merge(metric_updates)\n",
    "    state = state.replace(metrics=metrics)\n",
    "    return state\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "d5ddf0c5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Array(0.6835966, dtype=float32)"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# poincare_linear = PoincareLinear()\n",
    "    \n",
    "poincare_linear = PoincareLinear()\n",
    "train_state = create_train_state(poincare_linear, jax.random.PRNGKey(43), 1e-02)\n",
    "\n",
    "batch_x = poincare_ball_normal.sample()\n",
    "batch_y = real_fn_poincare.apply_fn(batch_x)\n",
    "\n",
    "train_state = compute_metrics(state=train_state, batch_x=batch_x, batch_y=batch_y)\n",
    "\n",
    "biases = train_state.params['bias_poincare']\n",
    "train_state = train_step(train_state, batch_x, batch_y)\n",
    "\n",
    "biases = train_state.params['bias_poincare']\n",
    "\n",
    "train_state = compute_metrics(state=train_state, batch_x=batch_x,batch_y=batch_y)\n",
    "train_state.metrics.compute()['loss']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "be4c2354",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 Loss: 0.06442015618085861\n",
      "100 Loss: 0.061408333480358124\n",
      "200 Loss: 0.058650337159633636\n",
      "300 Loss: 0.05613384395837784\n",
      "400 Loss: 0.05383966118097305\n",
      "500 Loss: 0.051750294864177704\n",
      "600 Loss: 0.049802880734205246\n",
      "700 Loss: 0.04801582545042038\n",
      "800 Loss: 0.04632076248526573\n",
      "900 Loss: 0.044779401272535324\n"
     ]
    }
   ],
   "source": [
    "n_iter = 1000\n",
    "\n",
    "for i in range(n_iter):\n",
    "    \n",
    "    batch_x = poincare_ball_normal.sample()\n",
    "    batch_y = real_fn_poincare.apply_fn(batch_x)\n",
    "    \n",
    "    train_state = train_step(train_state, batch_x, batch_y)\n",
    "    train_state = compute_metrics(state=train_state, batch_x=batch_x,batch_y=batch_y)\n",
    "    \n",
    "    loss = train_state.metrics.compute()['loss']\n",
    "    if i % 100 == 0:\n",
    "        print(f'{i} Loss: {loss}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67a08a51",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
