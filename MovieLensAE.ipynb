{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: livelossplot in /home/skholkin/projects/python_venv/lib/python3.10/site-packages (0.5.5)\n",
      "Requirement already satisfied: matplotlib in /home/skholkin/projects/python_venv/lib/python3.10/site-packages (from livelossplot) (3.6.3)\n",
      "Requirement already satisfied: bokeh in /home/skholkin/projects/python_venv/lib/python3.10/site-packages (from livelossplot) (3.1.0)\n",
      "Requirement already satisfied: pillow>=7.1.0 in /home/skholkin/projects/python_venv/lib/python3.10/site-packages (from bokeh->livelossplot) (9.4.0)\n",
      "Requirement already satisfied: tornado>=5.1 in /home/skholkin/projects/python_venv/lib/python3.10/site-packages (from bokeh->livelossplot) (6.2)\n",
      "Requirement already satisfied: PyYAML>=3.10 in /home/skholkin/projects/python_venv/lib/python3.10/site-packages (from bokeh->livelossplot) (6.0)\n",
      "Requirement already satisfied: contourpy>=1 in /home/skholkin/projects/python_venv/lib/python3.10/site-packages (from bokeh->livelossplot) (1.0.7)\n",
      "Requirement already satisfied: packaging>=16.8 in /home/skholkin/projects/python_venv/lib/python3.10/site-packages (from bokeh->livelossplot) (23.0)\n",
      "Requirement already satisfied: xyzservices>=2021.09.1 in /home/skholkin/projects/python_venv/lib/python3.10/site-packages (from bokeh->livelossplot) (2023.2.0)\n",
      "Requirement already satisfied: Jinja2>=2.9 in /home/skholkin/projects/python_venv/lib/python3.10/site-packages (from bokeh->livelossplot) (3.1.2)\n",
      "Requirement already satisfied: pandas>=1.2 in /home/skholkin/projects/python_venv/lib/python3.10/site-packages (from bokeh->livelossplot) (1.5.2)\n",
      "Requirement already satisfied: numpy>=1.16 in /home/skholkin/projects/python_venv/lib/python3.10/site-packages (from bokeh->livelossplot) (1.23.5)\n",
      "Requirement already satisfied: pyparsing>=2.2.1 in /home/skholkin/projects/python_venv/lib/python3.10/site-packages (from matplotlib->livelossplot) (3.0.9)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /home/skholkin/projects/python_venv/lib/python3.10/site-packages (from matplotlib->livelossplot) (4.38.0)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /home/skholkin/projects/python_venv/lib/python3.10/site-packages (from matplotlib->livelossplot) (2.8.2)\n",
      "Requirement already satisfied: cycler>=0.10 in /home/skholkin/projects/python_venv/lib/python3.10/site-packages (from matplotlib->livelossplot) (0.11.0)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /home/skholkin/projects/python_venv/lib/python3.10/site-packages (from matplotlib->livelossplot) (1.4.4)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /home/skholkin/projects/python_venv/lib/python3.10/site-packages (from Jinja2>=2.9->bokeh->livelossplot) (2.1.1)\n",
      "Requirement already satisfied: pytz>=2020.1 in /home/skholkin/projects/python_venv/lib/python3.10/site-packages (from pandas>=1.2->bokeh->livelossplot) (2022.7)\n",
      "Requirement already satisfied: six>=1.5 in /home/skholkin/projects/python_venv/lib/python3.10/site-packages (from python-dateutil>=2.7->matplotlib->livelossplot) (1.16.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install livelossplot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.autograd import Variable\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms, utils\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.pyplot as pyplot\n",
    "from livelossplot import PlotLosses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class MSEloss_with_Mask(nn.Module):\n",
    "  def __init__(self):\n",
    "    super(MSEloss_with_Mask,self).__init__()\n",
    "\n",
    "  def forward(self,inputs, targets):\n",
    "    # Masking into a vector of 1's and 0's.\n",
    "    mask = (targets!=0)\n",
    "    mask = mask.float()\n",
    "\n",
    "    # Actual number of ratings.\n",
    "    # Take max to avoid division by zero while calculating loss.\n",
    "    other = torch.Tensor([1.0])\n",
    "    number_ratings = torch.max(torch.sum(mask),other)\n",
    "    error = torch.sum(torch.mul(mask,torch.mul((targets-inputs),(targets-inputs))))\n",
    "    loss = error.div(number_ratings)\n",
    "    return loss[0]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset and DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class TestDataset(Dataset):\n",
    "    def __init__(self, test_file, transform=None):\n",
    "        self.data = pd.read_csv(test_file)\n",
    "        self.data = self.data.iloc[:,1:]\n",
    "        self.transform = transform\n",
    "        \n",
    "        if transform is not None:\n",
    "            self.data = self.transform(np.array(self.data))\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.data[0])\n",
    "    \n",
    "    def __getitem__(self, ind):\n",
    "        user_vector = self.data.data[0][ind]\n",
    "        \n",
    "        return user_vector\n",
    "    \n",
    "\n",
    "class TrainDataset(Dataset):\n",
    "    def __init__(self, train_file, transform=None):\n",
    "        self.data = pd.read_csv(train_file)\n",
    "        self.data = self.data.iloc[:,1:]\n",
    "        self.transform = transform\n",
    "        \n",
    "        if transform is not None:\n",
    "            self.data = self.transform(np.array(self.data))\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.data[0])\n",
    "    \n",
    "    def __getitem__(self, ind):\n",
    "        user_vector = self.data.data[0][ind]\n",
    "        \n",
    "        return user_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Length:  6039\n",
      "6th User Ratings:  tensor([4., 0., 0.,  ..., 0., 0., 0.], dtype=torch.float64)\n"
     ]
    }
   ],
   "source": [
    "transformations = transforms.Compose([transforms.ToTensor()])\n",
    "train_dat = TrainDataset('train_1m.csv', transformations)\n",
    "\n",
    "print(\"Training Length: \", train_dat.__len__())\n",
    "print(\"6th User Ratings: \", train_dat.__getitem__(7))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing Length:  6039\n",
      "6th User Ratings:  tensor([0., 0., 0.,  ..., 0., 0., 0.], dtype=torch.float64)\n"
     ]
    }
   ],
   "source": [
    "test_dat = TestDataset('test_1m.csv', transformations)\n",
    "print(\"Testing Length: \", test_dat.__len__())\n",
    "print(\"6th User Ratings: \", test_dat.__getitem__(4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n"
     ]
    }
   ],
   "source": [
    "batch_size = 64\n",
    "train_dl = DataLoader(dataset=train_dat, batch_size = batch_size, shuffle=False, num_workers = 1)\n",
    "\n",
    "test_dl = DataLoader(dataset=test_dat, batch_size=batch_size, shuffle=False, num_workers=1)\n",
    "\n",
    "use_cuda = torch.cuda.is_available()\n",
    "device = torch.device(\"cuda:0\" if use_cuda else \"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.nn.init as init\n",
    "\n",
    "\n",
    "def activation(input, type):\n",
    "  \n",
    "    if type.lower()=='selu':\n",
    "        return F.selu(input)\n",
    "    elif type.lower()=='elu':\n",
    "        return F.elu(input)\n",
    "    elif type.lower()=='relu':\n",
    "        return F.relu(input)\n",
    "    elif type.lower()=='relu6':\n",
    "        return F.relu6(input)\n",
    "    elif type.lower()=='lrelu':\n",
    "        return F.leaky_relu(input)\n",
    "    elif type.lower()=='tanh':\n",
    "        return F.tanh(input)\n",
    "    elif type.lower()=='sigmoid':\n",
    "        return F.sigmoid(input)\n",
    "    elif type.lower()=='swish':\n",
    "        return F.sigmoid(input)*input\n",
    "    elif type.lower()=='identity':\n",
    "        return input\n",
    "    else:\n",
    "        raise ValueError(\"Unknown non-Linearity Type\")\n",
    "\n",
    "\n",
    "class AutoEncoder(nn.Module):\n",
    "\n",
    "    def __init__(self, layer_sizes, nl_type='selu', is_constrained=True, dp_drop_prob=0.0, last_layer_activations=True):\n",
    "        \"\"\"\n",
    "        layer_sizes = size of each layer in the autoencoder model\n",
    "        For example: [10000, 1024, 512] will result in:\n",
    "            - encoder 2 layers: 10000x1024 and 1024x512. Representation layer (z) will be 512\n",
    "            - decoder 2 layers: 512x1024 and 1024x10000.\n",
    "        \n",
    "        nl_type = non-Linearity type (default: 'selu).\n",
    "        is_constrained = If true then the weights of encoder and decoder are tied.\n",
    "        dp_drop_prob = Dropout probability.\n",
    "        last_layer_activations = Whether to apply activation on last decoder layer.\n",
    "        \"\"\"\n",
    "\n",
    "        super(AutoEncoder, self).__init__()\n",
    "\n",
    "        self.layer_sizes = layer_sizes\n",
    "        self.nl_type = nl_type\n",
    "        self.is_constrained = is_constrained\n",
    "        self.dp_drop_prob = dp_drop_prob\n",
    "        self.last_layer_activations = last_layer_activations\n",
    "\n",
    "        if dp_drop_prob>0:\n",
    "            self.drop = nn.Dropout(dp_drop_prob)\n",
    "\n",
    "        self._last = len(layer_sizes) - 2\n",
    "\n",
    "        # Initaialize Weights\n",
    "        self.encoder_weights = nn.ParameterList( [nn.Parameter(torch.rand(layer_sizes[i+1], layer_sizes[i])) for i in range(len(layer_sizes) - 1)  ] )\n",
    "\n",
    "        # \"Xavier Initialization\" ( Understanding the Difficulty in training deep feed forward neural networks - by Glorot, X. & Bengio, Y. )\n",
    "        # ( Values are sampled from uniform distribution )\n",
    "        for weights in self.encoder_weights:\n",
    "            init.xavier_uniform_(weights)\n",
    "\n",
    "        # Encoder Bias\n",
    "        self.encoder_bias = nn.ParameterList( [nn.Parameter(torch.zeros(layer_sizes[i+1])) for i in range(len(layer_sizes) - 1) ] )\n",
    "\n",
    "        reverse_layer_sizes = list(reversed(layer_sizes)) \n",
    "        # reversed returns iterator\n",
    "\n",
    "\n",
    "        # Decoder Weights\n",
    "        if is_constrained == False:\n",
    "            self.decoder_weights = nn.ParameterList( [nn.Parameter(torch.rand(reverse_layer_sizes[i+1], reverse_layer_sizes[i])) for i in range(len(reverse_layer_sizes) - 1) ] )\n",
    "\n",
    "            for weights in self.decoder_weights:\n",
    "                init.xavier_uniform_(weights)\n",
    "\n",
    "        self.decoder_bias = nn.ParameterList( [nn.Parameter(torch.zeros(reverse_layer_sizes[i+1])) for i in range(len(reverse_layer_sizes) - 1) ] )\n",
    "\n",
    "\n",
    "\n",
    "    def encode(self,x):\n",
    "        for i,w in enumerate(self.encoder_weights):\n",
    "            x = F.linear(input=x, weight = w, bias = self.encoder_bias[i] )\n",
    "            x = activation(input=x, type=self.nl_type)\n",
    "\n",
    "        # Apply Dropout on the last layer\n",
    "        if self.dp_drop_prob > 0:\n",
    "            x = self.drop(x)\n",
    "\n",
    "        return x\n",
    "\n",
    "\n",
    "    def decode(self,x):\n",
    "        if self.is_constrained == True:\n",
    "            # Weights are tied\n",
    "            for i,w in zip(range(len(self.encoder_weights)),list(reversed(self.encoder_weights))):\n",
    "                x = F.linear(input=x, weight=w.t(), bias = self.decoder_bias[i] )\n",
    "                x = activation(input=x, type=self.nl_type if i != self._last or self.last_layer_activations else 'identity')\n",
    "\n",
    "        else:\n",
    "\n",
    "            for i,w in enumerate(self.decoder_weights):\n",
    "                x = F.linear(input=x, weight = w, bias = self.decoder_weights[i])\n",
    "                x = activation(input=x, type=self.nl_type if i != self._last or self.last_layer_activations else 'identity')\n",
    "\n",
    "        return x\n",
    "\n",
    "    def forward(self,x):\n",
    "        # Forward Pass\n",
    "        return self.decode(self.encode(x))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "layer_sizes = [3701, 512, 512, 1024]\n",
    "\n",
    "layer_sizes = [3701, 512, 256]\n",
    "model = AutoEncoder(layer_sizes=layer_sizes, nl_type='selu', is_constrained=True, dp_drop_prob=0.0, last_layer_activations=False)\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.0001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "criterion = MSEloss_with_Mask()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, criterion, optimizer, train_dl, test_dl, num_epochs=40):\n",
    "  # We will run for 40 epochs\n",
    "  liveloss = PlotLosses()\n",
    "  for epoch in range(num_epochs):\n",
    "    train_loss, valid_loss = [], []\n",
    "    logs = {}\n",
    "    prefix = ''\n",
    "\n",
    "    # Training Part\n",
    "    model.train()\n",
    "    for i, data in enumerate(train_dl, 0):\n",
    "      # Get the inputs\n",
    "      inputs = labels = data\n",
    "#       inputs = inputs.cuda()\n",
    "#       labels = labels.cuda()\n",
    "\n",
    "      inputs = inputs.float()\n",
    "      labels = labels.float()\n",
    "\n",
    "      # zero the parameter gradients\n",
    "      optimizer.zero_grad()\n",
    "      # forward + backward + optimize\n",
    "      outputs = model(inputs)\n",
    "#       outputs = outputs.cuda()\n",
    "#       print(outputs.device, labels.device)\n",
    "#       print(criterion.device)\n",
    "      loss = criterion(outputs, labels)\n",
    "      loss.backward()\n",
    "      optimizer.step()\n",
    "\n",
    "      ## -> Iterative Dense Output Re-feeding <- ##\n",
    "      \n",
    "      # Add a \"for\" loop to iterate as much you want\n",
    "      \n",
    "      # Zero the gradiants\n",
    "      optimizer.zero_grad()\n",
    "      \n",
    "      # Important -> detach() the output, to avoid unecessary construction of \n",
    "      # the computational graph\n",
    "      outputs = model(outputs.detach())\n",
    "#       outputs = outputs.cuda() \n",
    "      \n",
    "      loss = criterion(outputs, labels)\n",
    "      loss.backward()\n",
    "      optimizer.step()\n",
    "\n",
    "      train_loss.append(loss.item())\n",
    "      logs[prefix + 'MMSE loss'] = loss.item()\n",
    "\n",
    "    for i, data in enumerate(test_dl, 0):\n",
    "      model.eval()\n",
    "      inputs = labels = data\n",
    "#       inputs = inputs.cuda()\n",
    "#       labels = labels.cuda()\n",
    "\n",
    "      inputs = inputs.float()\n",
    "      labels = labels.float()\n",
    "\n",
    "      outputs = model(inputs)\n",
    "#       outputs = outputs.cuda()\n",
    "      loss = criterion(outputs, labels)\n",
    "\n",
    "      valid_loss.append(loss.item())\n",
    "      prefix = 'val_'\n",
    "      logs[prefix + 'MMSE loss'] = loss.item()\n",
    "\n",
    "#     hlr2_tr_loss.append(np.mean(train_loss))\n",
    "#     hlr2_val_loss.append(np.mean(valid_loss))\n",
    "    liveloss.update(logs)\n",
    "    liveloss.draw()\n",
    "    print (\"Epoch:\", epoch+1, \" Training Loss: \", np.mean(train_loss), \" Valid Loss: \", np.mean(valid_loss))\n",
    "    if epoch == num_epochs -1:\n",
    "      return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1200x800 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MMSE loss\n",
      "\ttraining         \t (min:    0.833, max:    1.030, cur:    0.833)\n",
      "\tvalidation       \t (min:    0.766, max:    0.958, cur:    0.766)\n",
      "Epoch: 3  Training Loss:  0.9868067101428383  Valid Loss:  0.904401619810807\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[78], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcriterion\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_dl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest_dl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m40\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[77], line 28\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(model, criterion, optimizer, train_dl, test_dl, num_epochs)\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[38;5;66;03m#       outputs = outputs.cuda()\u001b[39;00m\n\u001b[1;32m     25\u001b[0m \u001b[38;5;66;03m#       print(outputs.device, labels.device)\u001b[39;00m\n\u001b[1;32m     26\u001b[0m \u001b[38;5;66;03m#       print(criterion.device)\u001b[39;00m\n\u001b[1;32m     27\u001b[0m       loss \u001b[38;5;241m=\u001b[39m criterion(outputs, labels)\n\u001b[0;32m---> 28\u001b[0m       \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     29\u001b[0m       optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[1;32m     31\u001b[0m       \u001b[38;5;66;03m## -> Iterative Dense Output Re-feeding <- ##\u001b[39;00m\n\u001b[1;32m     32\u001b[0m       \n\u001b[1;32m     33\u001b[0m       \u001b[38;5;66;03m# Add a \"for\" loop to iterate as much you want\u001b[39;00m\n\u001b[1;32m     34\u001b[0m       \n\u001b[1;32m     35\u001b[0m       \u001b[38;5;66;03m# Zero the gradiants\u001b[39;00m\n",
      "File \u001b[0;32m~/projects/python_venv/lib/python3.10/site-packages/torch/_tensor.py:488\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    478\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    479\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    480\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    481\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    486\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[1;32m    487\u001b[0m     )\n\u001b[0;32m--> 488\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    489\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[1;32m    490\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/projects/python_venv/lib/python3.10/site-packages/torch/autograd/__init__.py:197\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    192\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[1;32m    194\u001b[0m \u001b[38;5;66;03m# The reason we repeat same the comment below is that\u001b[39;00m\n\u001b[1;32m    195\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    196\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 197\u001b[0m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    198\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    199\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "out = train(model, criterion, optimizer, train_dl, test_dl, 40)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculating Recall and HR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.14587737843551796"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "def get_HR(model, train_loader, test_loader, top_n=10):\n",
    "\n",
    "    counter, hits = 0, 0\n",
    "    r = []\n",
    "    for i, (batch_train, batch_val) in enumerate(zip(iter(train_loader), iter(test_loader))):\n",
    "        delta = batch_val - batch_train\n",
    "        predicted_val = model(batch_train.float()) - batch_train\n",
    "        for k in range(delta.shape[0]):\n",
    "            non_zero_ind = np.nonzero(delta[k])\n",
    "            if len(non_zero_ind) > 0:\n",
    "                counter += 1\n",
    "                diff = set([int(item) for item  in np.argpartition(np.array(predicted_val.detach()[k]), -top_n)[-top_n:]]) - (set([int(item) for item in non_zero_ind]))\n",
    "                \n",
    "                if len(diff) < top_n:\n",
    "                    hits += 1\n",
    "    return hits / counter\n",
    "\n",
    "get_HR(model, train_dl, test_dl)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.043463207268371974"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "def get_recall(model, train_loader, test_loader, top_n=50):\n",
    "\n",
    "    counter, hits = 0, 0\n",
    "    r = []\n",
    "    for i, (batch_train, batch_val) in enumerate(zip(iter(train_loader), iter(test_loader))):\n",
    "        delta = batch_val - batch_train\n",
    "        predicted_val = model(batch_train.float()) - batch_train\n",
    "        for k in range(delta.shape[0]):\n",
    "            non_zero_ind = np.nonzero(delta[k])\n",
    "            if len(non_zero_ind) > 0:\n",
    "                counter += 1\n",
    "#                 print(non_zero_ind)\n",
    "                diff = set([int(item) for item  in np.argpartition(np.array(predicted_val.detach()[k]), -top_n)[-top_n:]]).intersection(set([int(item) for item in non_zero_ind]))\n",
    "                \n",
    "#                 diff = set([int(item) for item  in np.argpartition(np.array(predicted_val.detach()[k]), -top_n)[-top_n:]]) - set([int(non_zero_ind[0])])\n",
    "        \n",
    "#                 diff = set([int(item) for item  in np.argpartition(np.array(predicted_val.detach()[k]), -top_n)[-top_n:]]) - set([int(non_zero_ind[0])])\n",
    "                r.append(len(diff) / len(non_zero_ind))\n",
    "                if len(diff) < top_n:\n",
    "                    hits += 1\n",
    "    return np.mean(r)\n",
    "    return hits / counter\n",
    "        \n",
    "get_recall(model, train_dl, test_dl)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==========================================================================================\n",
      "Layer (type:depth-idx)                   Output Shape              Param #\n",
      "==========================================================================================\n",
      "AutoEncoder                              [64, 3701]                2,688,117\n",
      "==========================================================================================\n",
      "Total params: 2,688,117\n",
      "Trainable params: 2,688,117\n",
      "Non-trainable params: 0\n",
      "Total mult-adds (M): 0\n",
      "==========================================================================================\n",
      "Input size (MB): 0.95\n",
      "Forward/backward pass size (MB): 0.00\n",
      "Params size (MB): 0.00\n",
      "Estimated Total Size (MB): 0.95\n",
      "==========================================================================================\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "==========================================================================================\n",
       "Layer (type:depth-idx)                   Output Shape              Param #\n",
       "==========================================================================================\n",
       "AutoEncoder                              [64, 3701]                2,688,117\n",
       "==========================================================================================\n",
       "Total params: 2,688,117\n",
       "Trainable params: 2,688,117\n",
       "Non-trainable params: 0\n",
       "Total mult-adds (M): 0\n",
       "==========================================================================================\n",
       "Input size (MB): 0.95\n",
       "Forward/backward pass size (MB): 0.00\n",
       "Params size (MB): 0.00\n",
       "Estimated Total Size (MB): 0.95\n",
       "=========================================================================================="
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# from torchsummary import summary\n",
    "\n",
    "# summary(model, (64, 3701), verbose=2)\n",
    "from torchinfo import summary\n",
    "\n",
    "summary(model, input_size=(batch_size, 3701), verbose=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Jax model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/skholkin/projects/python_venv/lib/python3.10/site-packages/flax/core/frozen_dict.py:169: FutureWarning: jax.tree_util.register_keypaths is deprecated, and will be removed in a future release. Please use `register_pytree_with_keys()` instead.\n",
      "  jax.tree_util.register_keypaths(\n",
      "No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)\n"
     ]
    }
   ],
   "source": [
    "import jax\n",
    "import jax.numpy as jnp\n",
    "from flax import linen as nn\n",
    "from flax import struct\n",
    "\n",
    "import optax\n",
    "\n",
    "import flax\n",
    "import numpy as np\n",
    "from flax import struct \n",
    "from clu import metrics\n",
    "from dataclasses import field\n",
    "from functools import partial\n",
    "from typing import Any, Callable, Optional, Tuple, Union\n",
    "from flax import struct  \n",
    "\n",
    "from clu import metrics\n",
    "\n",
    "from chex import Array\n",
    "from flax import linen as nn\n",
    "from flax.linen.activation import sigmoid, tanh\n",
    "from flax.linen.dtypes import promote_dtype\n",
    "from flax.linen.initializers import orthogonal\n",
    "from flax.linen.linear import default_kernel_init\n",
    "from jax import numpy as jnp\n",
    "from jax import random, vmap\n",
    "from chex import Array\n",
    "from jax import lax\n",
    "from jax.nn.initializers import Initializer as Initializer\n",
    "from jax._src import dtypes\n",
    "\n",
    "from flax.training import train_state\n",
    "\n",
    "key = jax.random.PRNGKey(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "@struct.dataclass\n",
    "class Metrics(metrics.Collection):\n",
    "  loss: metrics.Average.from_output('loss')\n",
    "    \n",
    "class TrainState(train_state.TrainState):\n",
    "  metrics: Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[3m                                SimpleAE Summary                                \u001b[0m\n",
      "\n",
      "\u001b[1m \u001b[0m\u001b[1mpath   \u001b[0m\u001b[1m \u001b[0m\u001b[1m \u001b[0m\u001b[1mmodule  \u001b[0m\u001b[1m \u001b[0m\u001b[1m \u001b[0m\u001b[1minputs          \u001b[0m\u001b[1m \u001b[0m\u001b[1m \u001b[0m\u001b[1moutputs         \u001b[0m\u001b[1m \u001b[0m\u001b[1m \u001b[0m\u001b[1mparams           \u001b[0m\u001b[1m \u001b[0m\n",
      "\n",
      "          SimpleAE  \u001b[2mfloat32\u001b[0m[64,3701]  \u001b[2mfloat32\u001b[0m[64,3701]  Dense_0:          \n",
      "                                                          bias:           \n",
      "                                                        \u001b[2mfloat32\u001b[0m[512]      \n",
      "                                                          kernel:         \n",
      "                                                        \u001b[2mfloat32\u001b[0m[3701,512] \n",
      "                                                        Dense_1:          \n",
      "                                                          bias:           \n",
      "                                                        \u001b[2mfloat32\u001b[0m[256]      \n",
      "                                                          kernel:         \n",
      "                                                        \u001b[2mfloat32\u001b[0m[512,256]  \n",
      "                                                        Dense_2:          \n",
      "                                                          bias:           \n",
      "                                                        \u001b[2mfloat32\u001b[0m[512]      \n",
      "                                                          kernel:         \n",
      "                                                        \u001b[2mfloat32\u001b[0m[256,512]  \n",
      "                                                        Dense_3:          \n",
      "                                                          bias:           \n",
      "                                                        \u001b[2mfloat32\u001b[0m[3701]     \n",
      "                                                          kernel:         \n",
      "                                                        \u001b[2mfloat32\u001b[0m[512,3701] \n",
      "                                                                          \n",
      "                                                        \u001b[1m4,056,949 \u001b[0m\u001b[1;2m(16.2 \u001b[0m  \n",
      "                                                        \u001b[1;2mMB)\u001b[0m               \n",
      "\n",
      " Dense_0  Dense     \u001b[2mfloat32\u001b[0m[64,3701]  \u001b[2mfloat32\u001b[0m[64,512]   bias:             \n",
      "                                                        \u001b[2mfloat32\u001b[0m[512]      \n",
      "                                                        kernel:           \n",
      "                                                        \u001b[2mfloat32\u001b[0m[3701,512] \n",
      "                                                                          \n",
      "                                                        \u001b[1m1,895,424 \u001b[0m\u001b[1;2m(7.6 \u001b[0m   \n",
      "                                                        \u001b[1;2mMB)\u001b[0m               \n",
      "\n",
      " Dense_1  Dense     \u001b[2mfloat32\u001b[0m[64,512]   \u001b[2mfloat32\u001b[0m[64,256]   bias:             \n",
      "                                                        \u001b[2mfloat32\u001b[0m[256]      \n",
      "                                                        kernel:           \n",
      "                                                        \u001b[2mfloat32\u001b[0m[512,256]  \n",
      "                                                                          \n",
      "                                                        \u001b[1m131,328 \u001b[0m\u001b[1;2m(525.3 \u001b[0m   \n",
      "                                                        \u001b[1;2mKB)\u001b[0m               \n",
      "\n",
      " Dense_2  Dense     \u001b[2mfloat32\u001b[0m[64,256]   \u001b[2mfloat32\u001b[0m[64,512]   bias:             \n",
      "                                                        \u001b[2mfloat32\u001b[0m[512]      \n",
      "                                                        kernel:           \n",
      "                                                        \u001b[2mfloat32\u001b[0m[256,512]  \n",
      "                                                                          \n",
      "                                                        \u001b[1m131,584 \u001b[0m\u001b[1;2m(526.3 \u001b[0m   \n",
      "                                                        \u001b[1;2mKB)\u001b[0m               \n",
      "\n",
      " Dense_3  Dense     \u001b[2mfloat32\u001b[0m[64,512]   \u001b[2mfloat32\u001b[0m[64,3701]  bias:             \n",
      "                                                        \u001b[2mfloat32\u001b[0m[3701]     \n",
      "                                                        kernel:           \n",
      "                                                        \u001b[2mfloat32\u001b[0m[512,3701] \n",
      "                                                                          \n",
      "                                                        \u001b[1m1,898,613 \u001b[0m\u001b[1;2m(7.6 \u001b[0m   \n",
      "                                                        \u001b[1;2mMB)\u001b[0m               \n",
      "\n",
      "\u001b[1m \u001b[0m\u001b[1m       \u001b[0m\u001b[1m \u001b[0m\u001b[1m \u001b[0m\u001b[1m        \u001b[0m\u001b[1m \u001b[0m\u001b[1m \u001b[0m\u001b[1m                \u001b[0m\u001b[1m \u001b[0m\u001b[1m \u001b[0m\u001b[1m           Total\u001b[0m\u001b[1m \u001b[0m\u001b[1m \u001b[0m\u001b[1m8,113,898 \u001b[0m\u001b[1;2m(32.5 \u001b[0m\u001b[1m \u001b[0m\u001b[1m \u001b[0m\n",
      "\u001b[1m         \u001b[0m\u001b[1m          \u001b[0m\u001b[1m                  \u001b[0m\u001b[1m                  \u001b[0m\u001b[1m \u001b[0m\u001b[1;2mMB)\u001b[0m\u001b[1m              \u001b[0m\u001b[1m \u001b[0m\n",
      "\n",
      "\u001b[1m                                                                                \u001b[0m\n",
      "\u001b[1m                     Total Parameters: 8,113,898 \u001b[0m\u001b[1;2m(32.5 MB)\u001b[0m\u001b[1m                      \u001b[0m\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "dim = next(iter(train_dl)).shape[1]\n",
    "\n",
    "hidden_dim = 1000\n",
    "\n",
    "layer_sizes = [3701, 512, 256]\n",
    "\n",
    "class SimpleAE(nn.Module):\n",
    "    \n",
    "  @nn.compact\n",
    "  def __call__(self, x):\n",
    "    x = nn.Dense(layer_sizes[1])(x)\n",
    "    x = nn.selu(x)\n",
    "    x = nn.Dense(layer_sizes[2])(x)\n",
    "    x = nn.selu(x)\n",
    "    x = nn.Dense(layer_sizes[1])(x)\n",
    "    x = nn.selu(x)\n",
    "    x = nn.Dense(layer_sizes[0])(x)\n",
    "    return x\n",
    "\n",
    "autoencoder = SimpleAE()\n",
    "print(autoencoder.tabulate(jax.random.PRNGKey(42), jnp.ones((batch_size, dim))))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def create_train_state(module, rng, learning_rate):\n",
    "  \"\"\"Creates an initial `TrainState`.\"\"\"\n",
    "  params = module.init(rng, jnp.ones([batch_size, dim]))['params'] # initialize parameters by passing a template image\n",
    "  tx = optax.sgd(learning_rate)\n",
    "  return TrainState.create(\n",
    "      apply_fn=module.apply, params=params, tx=tx,\n",
    "      metrics=Metrics.empty())\n",
    "\n",
    "train_state = create_train_state(autoencoder, jax.random.PRNGKey(43), 1e-3)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def MSEMasked(inputs, targets): # Masking into a vector of 1's and 0's.\n",
    "    mask = (targets!=0)\n",
    "    mask = jnp.float32(mask)\n",
    "\n",
    "    # Actual number of ratings.\n",
    "    # Take max to avoid division by zero while calculating loss.\n",
    "    other = jnp.array(1.0)\n",
    "    number_ratings = jnp.maximum(jnp.sum(mask), other)\n",
    "    error = jnp.sum(mask * (targets - inputs) ** 2)\n",
    "    loss = error / number_ratings\n",
    "    return loss\n",
    "    \n",
    "    \n",
    "@jax.jit\n",
    "def train_step(state, batch):\n",
    "  \"\"\"Train for a single step.\"\"\"\n",
    "\n",
    "  def loss_fn(params):\n",
    "    x_rec = state.apply_fn({'params': params}, batch)\n",
    "    loss = MSEMasked(batch, x_rec)\n",
    "    return loss\n",
    "\n",
    "  grad_fn = jax.grad(loss_fn)\n",
    "  grads = grad_fn(state.params)\n",
    "  state = state.apply_gradients(grads=grads)\n",
    "  return state\n",
    "\n",
    "@jax.jit\n",
    "def compute_metrics(*, state, batch):\n",
    "    x_rec = state.apply_fn({'params': state.params}, batch)\n",
    "    loss = ((x_rec - batch) ** 2).mean()\n",
    "    metric_updates = state.metrics.single_from_model_output(loss=loss)\n",
    "    metrics = state.metrics.merge(metric_updates)\n",
    "    state = state.replace(metrics=metrics)\n",
    "    return state\n",
    "\n",
    "@jax.jit\n",
    "def predict(state, batch):\n",
    "    x_rec = state.apply_fn({'params': state.params}, batch)\n",
    "    return x_rec\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Array(1.2523139, dtype=float32)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "sample_batch = jnp.array(next(iter(train_dl)))\n",
    "\n",
    "train_step(train_state, sample_batch)\n",
    "\n",
    "train_state = compute_metrics(state=train_state, batch=sample_batch)\n",
    "train_state.metrics.compute()['loss']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0 Iter 94 Metrics: 1.2654837369918823\n",
      "Epoch: 1 Iter 94 Metrics: 1.2492533922195435\n",
      "Epoch: 2 Iter 94 Metrics: 1.231633186340332\n",
      "Epoch: 3 Iter 94 Metrics: 1.2188681364059448\n",
      "Epoch: 4 Iter 94 Metrics: 1.2087388038635254\n",
      "Epoch: 5 Iter 94 Metrics: 1.200228214263916\n",
      "Epoch: 6 Iter 94 Metrics: 1.192807674407959\n",
      "Epoch: 7 Iter 94 Metrics: 1.1861687898635864\n",
      "Epoch: 8 Iter 94 Metrics: 1.180118441581726\n",
      "Epoch: 9 Iter 94 Metrics: 1.1745266914367676\n",
      "Epoch: 10 Iter 94 Metrics: 1.1693040132522583\n",
      "Epoch: 11 Iter 94 Metrics: 1.1643840074539185\n",
      "Epoch: 12 Iter 94 Metrics: 1.1597177982330322\n",
      "Epoch: 13 Iter 94 Metrics: 1.1552667617797852\n",
      "Epoch: 14 Iter 94 Metrics: 1.1510013341903687\n",
      "Epoch: 15 Iter 94 Metrics: 1.1468963623046875\n",
      "Epoch: 16 Iter 94 Metrics: 1.1429322957992554\n",
      "Epoch: 17 Iter 94 Metrics: 1.1390928030014038\n",
      "Epoch: 18 Iter 94 Metrics: 1.1353641748428345\n",
      "Epoch: 19 Iter 94 Metrics: 1.1317347288131714\n",
      "Epoch: 20 Iter 94 Metrics: 1.128193736076355\n",
      "Epoch: 21 Iter 94 Metrics: 1.1247330904006958\n",
      "Epoch: 22 Iter 94 Metrics: 1.1213457584381104\n",
      "Epoch: 23 Iter 94 Metrics: 1.1180245876312256\n",
      "Epoch: 24 Iter 94 Metrics: 1.1147644519805908\n",
      "Epoch: 25 Iter 94 Metrics: 1.1115591526031494\n",
      "Epoch: 26 Iter 94 Metrics: 1.1084043979644775\n",
      "Epoch: 27 Iter 94 Metrics: 1.1052956581115723\n",
      "Epoch: 28 Iter 94 Metrics: 1.102230191230774\n",
      "Epoch: 29 Iter 94 Metrics: 1.0992045402526855\n",
      "Epoch: 30 Iter 94 Metrics: 1.0962162017822266\n",
      "Epoch: 31 Iter 94 Metrics: 1.0932621955871582\n",
      "Epoch: 32 Iter 94 Metrics: 1.0903401374816895\n",
      "Epoch: 33 Iter 94 Metrics: 1.0874481201171875\n",
      "Epoch: 34 Iter 94 Metrics: 1.0845847129821777\n",
      "Epoch: 35 Iter 94 Metrics: 1.0817480087280273\n",
      "Epoch: 36 Iter 94 Metrics: 1.078936219215393\n",
      "Epoch: 37 Iter 94 Metrics: 1.0761481523513794\n",
      "Epoch: 38 Iter 94 Metrics: 1.0733829736709595\n",
      "Epoch: 39 Iter 94 Metrics: 1.0706391334533691\n",
      "Epoch: 40 Iter 94 Metrics: 1.0679161548614502\n",
      "Epoch: 41 Iter 94 Metrics: 1.065212607383728\n",
      "Epoch: 42 Iter 94 Metrics: 1.062528371810913\n",
      "Epoch: 43 Iter 94 Metrics: 1.059862494468689\n",
      "Epoch: 44 Iter 94 Metrics: 1.057214379310608\n",
      "Epoch: 45 Iter 94 Metrics: 1.054583191871643\n",
      "Epoch: 46 Iter 94 Metrics: 1.0519686937332153\n",
      "Epoch: 47 Iter 94 Metrics: 1.049370527267456\n",
      "Epoch: 48 Iter 94 Metrics: 1.0467883348464966\n",
      "Epoch: 49 Iter 94 Metrics: 1.044222116470337\n",
      "Epoch: 50 Iter 94 Metrics: 1.0416712760925293\n",
      "Epoch: 51 Iter 94 Metrics: 1.0391355752944946\n",
      "Epoch: 52 Iter 94 Metrics: 1.0366148948669434\n",
      "Epoch: 53 Iter 94 Metrics: 1.0341094732284546\n",
      "Epoch: 54 Iter 94 Metrics: 1.0316184759140015\n",
      "Epoch: 55 Iter 94 Metrics: 1.0291422605514526\n",
      "Epoch: 56 Iter 94 Metrics: 1.0266807079315186\n",
      "Epoch: 57 Iter 94 Metrics: 1.0242335796356201\n",
      "Epoch: 58 Iter 94 Metrics: 1.021801233291626\n",
      "Epoch: 59 Iter 94 Metrics: 1.019383430480957\n",
      "Epoch: 60 Iter 94 Metrics: 1.0169798135757446\n",
      "Epoch: 61 Iter 94 Metrics: 1.014590859413147\n",
      "Epoch: 62 Iter 94 Metrics: 1.0122164487838745\n",
      "Epoch: 63 Iter 94 Metrics: 1.0098570585250854\n",
      "Epoch: 64 Iter 94 Metrics: 1.0075122117996216\n",
      "Epoch: 65 Iter 94 Metrics: 1.0051817893981934\n",
      "Epoch: 66 Iter 94 Metrics: 1.0028657913208008\n",
      "Epoch: 67 Iter 94 Metrics: 1.0005643367767334\n",
      "Epoch: 68 Iter 94 Metrics: 0.9982775449752808\n",
      "Epoch: 69 Iter 94 Metrics: 0.9960053563117981\n",
      "Epoch: 70 Iter 94 Metrics: 0.9937475323677063\n",
      "Epoch: 71 Iter 94 Metrics: 0.991504967212677\n",
      "Epoch: 72 Iter 94 Metrics: 0.9892773032188416\n",
      "Epoch: 73 Iter 94 Metrics: 0.9870643019676208\n",
      "Epoch: 74 Iter 94 Metrics: 0.984866201877594\n",
      "Epoch: 75 Iter 94 Metrics: 0.9826828241348267\n",
      "Epoch: 76 Iter 94 Metrics: 0.9805145859718323\n",
      "Epoch: 77 Iter 94 Metrics: 0.9783613085746765\n",
      "Epoch: 78 Iter 94 Metrics: 0.9762229919433594\n",
      "Epoch: 79 Iter 94 Metrics: 0.9740994572639465\n",
      "Epoch: 80 Iter 94 Metrics: 0.9719910621643066\n",
      "Epoch: 81 Iter 94 Metrics: 0.969897985458374\n",
      "Epoch: 82 Iter 94 Metrics: 0.9678200483322144\n",
      "Epoch: 83 Iter 94 Metrics: 0.9657572507858276\n",
      "Epoch: 84 Iter 94 Metrics: 0.9637089967727661\n",
      "Epoch: 85 Iter 94 Metrics: 0.9616758823394775\n",
      "Epoch: 86 Iter 94 Metrics: 0.9596579074859619\n",
      "Epoch: 87 Iter 94 Metrics: 0.9576546549797058\n",
      "Epoch: 88 Iter 94 Metrics: 0.9556663632392883\n",
      "Epoch: 89 Iter 94 Metrics: 0.9536928534507751\n",
      "Epoch: 90 Iter 94 Metrics: 0.9517340660095215\n",
      "Epoch: 91 Iter 94 Metrics: 0.9497899413108826\n",
      "Epoch: 92 Iter 94 Metrics: 0.947860598564148\n",
      "Epoch: 93 Iter 94 Metrics: 0.9459460973739624\n",
      "Epoch: 94 Iter 94 Metrics: 0.9440459609031677\n",
      "Epoch: 95 Iter 94 Metrics: 0.9421600699424744\n",
      "Epoch: 96 Iter 94 Metrics: 0.9402886033058167\n",
      "Epoch: 97 Iter 94 Metrics: 0.9384316205978394\n",
      "Epoch: 98 Iter 94 Metrics: 0.9365888237953186\n",
      "Epoch: 99 Iter 94 Metrics: 0.9347602128982544\n",
      "Epoch: 100 Iter 94 Metrics: 0.9329450726509094\n",
      "Epoch: 101 Iter 94 Metrics: 0.9311440587043762\n",
      "Epoch: 102 Iter 94 Metrics: 0.9293568134307861\n",
      "Epoch: 103 Iter 94 Metrics: 0.927582859992981\n",
      "Epoch: 104 Iter 94 Metrics: 0.9258226752281189\n",
      "Epoch: 105 Iter 94 Metrics: 0.9240760207176208\n",
      "Epoch: 106 Iter 94 Metrics: 0.922342836856842\n",
      "Epoch: 107 Iter 94 Metrics: 0.920622706413269\n",
      "Epoch: 108 Iter 94 Metrics: 0.9189158082008362\n",
      "Epoch: 109 Iter 94 Metrics: 0.9172219038009644\n",
      "Epoch: 110 Iter 94 Metrics: 0.9155409336090088\n",
      "Epoch: 111 Iter 94 Metrics: 0.9138728380203247\n",
      "Epoch: 112 Iter 94 Metrics: 0.9122170805931091\n",
      "Epoch: 113 Iter 94 Metrics: 0.9105738997459412\n",
      "Epoch: 114 Iter 94 Metrics: 0.9089427590370178\n",
      "Epoch: 115 Iter 94 Metrics: 0.9073240756988525\n",
      "Epoch: 116 Iter 94 Metrics: 0.9057179093360901\n",
      "Epoch: 117 Iter 94 Metrics: 0.9041229486465454\n",
      "Epoch: 118 Iter 94 Metrics: 0.9025402069091797\n",
      "Epoch: 119 Iter 94 Metrics: 0.9009694457054138\n",
      "Epoch: 120 Iter 94 Metrics: 0.8994104266166687\n",
      "Epoch: 121 Iter 94 Metrics: 0.8978630304336548\n",
      "Epoch: 122 Iter 94 Metrics: 0.8963268995285034\n",
      "Epoch: 123 Iter 94 Metrics: 0.894801914691925\n",
      "Epoch: 124 Iter 94 Metrics: 0.893288254737854\n",
      "Epoch: 125 Iter 94 Metrics: 0.8917858600616455\n",
      "Epoch: 126 Iter 94 Metrics: 0.8902943134307861\n",
      "Epoch: 127 Iter 94 Metrics: 0.8888137340545654\n",
      "Epoch: 128 Iter 94 Metrics: 0.88734370470047\n",
      "Epoch: 129 Iter 94 Metrics: 0.8858848810195923\n",
      "Epoch: 130 Iter 94 Metrics: 0.8844363689422607\n",
      "Epoch: 131 Iter 94 Metrics: 0.882998526096344\n",
      "Epoch: 132 Iter 94 Metrics: 0.8815706968307495\n",
      "Epoch: 133 Iter 94 Metrics: 0.880153477191925\n",
      "Epoch: 134 Iter 94 Metrics: 0.8787461519241333\n",
      "Epoch: 135 Iter 94 Metrics: 0.877348780632019\n",
      "Epoch: 136 Iter 94 Metrics: 0.8759616613388062\n",
      "Epoch: 137 Iter 94 Metrics: 0.8745842576026917\n",
      "Epoch: 138 Iter 94 Metrics: 0.8732166290283203\n",
      "Epoch: 139 Iter 94 Metrics: 0.8718588352203369\n",
      "Epoch: 140 Iter 94 Metrics: 0.8705103993415833\n",
      "Epoch: 141 Iter 94 Metrics: 0.8691713213920593\n",
      "Epoch: 142 Iter 94 Metrics: 0.867841899394989\n",
      "Epoch: 143 Iter 94 Metrics: 0.8665220737457275\n",
      "Epoch: 144 Iter 94 Metrics: 0.865210771560669\n",
      "Epoch: 145 Iter 94 Metrics: 0.8639088869094849\n",
      "Epoch: 146 Iter 94 Metrics: 0.8626161813735962\n",
      "Epoch: 147 Iter 94 Metrics: 0.8613321781158447\n",
      "Epoch: 148 Iter 94 Metrics: 0.8600568771362305\n",
      "Epoch: 149 Iter 94 Metrics: 0.8587905168533325\n",
      "Epoch: 150 Iter 94 Metrics: 0.8575325608253479\n",
      "Epoch: 151 Iter 94 Metrics: 0.85628342628479\n",
      "Epoch: 152 Iter 94 Metrics: 0.855042576789856\n",
      "Epoch: 153 Iter 94 Metrics: 0.8538098931312561\n",
      "Epoch: 154 Iter 94 Metrics: 0.8525857329368591\n",
      "Epoch: 155 Iter 94 Metrics: 0.8513697981834412\n",
      "Epoch: 156 Iter 94 Metrics: 0.8501617908477783\n",
      "Epoch: 157 Iter 94 Metrics: 0.8489616513252258\n",
      "Epoch: 158 Iter 94 Metrics: 0.8477694988250732\n",
      "Epoch: 159 Iter 94 Metrics: 0.8465854525566101\n",
      "Epoch: 160 Iter 94 Metrics: 0.8454088568687439\n",
      "Epoch: 161 Iter 94 Metrics: 0.8442400693893433\n",
      "Epoch: 162 Iter 94 Metrics: 0.8430789113044739\n",
      "Epoch: 163 Iter 94 Metrics: 0.8419253826141357\n",
      "Epoch: 164 Iter 94 Metrics: 0.8407793045043945\n",
      "Epoch: 165 Iter 94 Metrics: 0.8396404385566711\n",
      "Epoch: 166 Iter 94 Metrics: 0.8385085463523865\n",
      "Epoch: 167 Iter 94 Metrics: 0.8373839259147644\n",
      "Epoch: 168 Iter 94 Metrics: 0.8362664580345154\n",
      "Epoch: 169 Iter 94 Metrics: 0.8351560235023499\n",
      "Epoch: 170 Iter 94 Metrics: 0.8340529203414917\n",
      "Epoch: 171 Iter 94 Metrics: 0.8329566717147827\n",
      "Epoch: 172 Iter 94 Metrics: 0.8318671584129333\n",
      "Epoch: 173 Iter 94 Metrics: 0.830784261226654\n",
      "Epoch: 174 Iter 94 Metrics: 0.8297080993652344\n",
      "Epoch: 175 Iter 94 Metrics: 0.8286385536193848\n",
      "Epoch: 176 Iter 94 Metrics: 0.8275755643844604\n",
      "Epoch: 177 Iter 94 Metrics: 0.8265190124511719\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 178 Iter 94 Metrics: 0.8254687190055847\n",
      "Epoch: 179 Iter 94 Metrics: 0.824424684047699\n",
      "Epoch: 180 Iter 94 Metrics: 0.8233870267868042\n",
      "Epoch: 181 Iter 94 Metrics: 0.8223552703857422\n",
      "Epoch: 182 Iter 94 Metrics: 0.8213297128677368\n",
      "Epoch: 183 Iter 94 Metrics: 0.820310115814209\n",
      "Epoch: 184 Iter 94 Metrics: 0.8192968368530273\n",
      "Epoch: 185 Iter 94 Metrics: 0.8182895183563232\n",
      "Epoch: 186 Iter 94 Metrics: 0.8172877430915833\n",
      "Epoch: 187 Iter 94 Metrics: 0.8162919282913208\n",
      "Epoch: 188 Iter 94 Metrics: 0.8153018951416016\n",
      "Epoch: 189 Iter 94 Metrics: 0.8143177628517151\n",
      "Epoch: 190 Iter 94 Metrics: 0.8133392930030823\n",
      "Epoch: 191 Iter 94 Metrics: 0.8123665452003479\n",
      "Epoch: 192 Iter 94 Metrics: 0.8113994598388672\n",
      "Epoch: 193 Iter 94 Metrics: 0.8104376792907715\n",
      "Epoch: 194 Iter 94 Metrics: 0.8094813823699951\n",
      "Epoch: 195 Iter 94 Metrics: 0.8085306882858276\n",
      "Epoch: 196 Iter 94 Metrics: 0.8075851202011108\n",
      "Epoch: 197 Iter 94 Metrics: 0.806644856929779\n",
      "Epoch: 198 Iter 94 Metrics: 0.8057100176811218\n",
      "Epoch: 199 Iter 94 Metrics: 0.8047805428504944\n",
      "Epoch: 200 Iter 94 Metrics: 0.8038563132286072\n",
      "Epoch: 201 Iter 94 Metrics: 0.8029373288154602\n",
      "Epoch: 202 Iter 94 Metrics: 0.8020234107971191\n",
      "Epoch: 203 Iter 94 Metrics: 0.8011143803596497\n",
      "Epoch: 204 Iter 94 Metrics: 0.8002105951309204\n",
      "Epoch: 205 Iter 94 Metrics: 0.7993117570877075\n",
      "Epoch: 206 Iter 94 Metrics: 0.7984178066253662\n",
      "Epoch: 207 Iter 94 Metrics: 0.7975288033485413\n",
      "Epoch: 208 Iter 94 Metrics: 0.7966448068618774\n",
      "Epoch: 209 Iter 94 Metrics: 0.7957654595375061\n",
      "Epoch: 210 Iter 94 Metrics: 0.7948910593986511\n",
      "Epoch: 211 Iter 94 Metrics: 0.7940215468406677\n",
      "Epoch: 212 Iter 94 Metrics: 0.7931566834449768\n",
      "Epoch: 213 Iter 94 Metrics: 0.7922965884208679\n",
      "Epoch: 214 Iter 94 Metrics: 0.7914408445358276\n",
      "Epoch: 215 Iter 94 Metrics: 0.7905897498130798\n",
      "Epoch: 216 Iter 94 Metrics: 0.7897432446479797\n",
      "Epoch: 217 Iter 94 Metrics: 0.7889014482498169\n",
      "Epoch: 218 Iter 94 Metrics: 0.7880640625953674\n",
      "Epoch: 219 Iter 94 Metrics: 0.7872311472892761\n",
      "Epoch: 220 Iter 94 Metrics: 0.7864027619361877\n",
      "Epoch: 221 Iter 94 Metrics: 0.7855790257453918\n",
      "Epoch: 222 Iter 94 Metrics: 0.7847588062286377\n",
      "Epoch: 223 Iter 94 Metrics: 0.7839431166648865\n",
      "Epoch: 224 Iter 94 Metrics: 0.7831318378448486\n",
      "Epoch: 225 Iter 94 Metrics: 0.7823247313499451\n",
      "Epoch: 226 Iter 94 Metrics: 0.781522274017334\n",
      "Epoch: 227 Iter 94 Metrics: 0.7807236909866333\n",
      "Epoch: 228 Iter 94 Metrics: 0.7799291014671326\n",
      "Epoch: 229 Iter 94 Metrics: 0.7791390419006348\n",
      "Epoch: 230 Iter 94 Metrics: 0.7783529758453369\n",
      "Epoch: 231 Iter 94 Metrics: 0.7775708436965942\n",
      "Epoch: 232 Iter 94 Metrics: 0.7767927050590515\n",
      "Epoch: 233 Iter 94 Metrics: 0.7760186791419983\n",
      "Epoch: 234 Iter 94 Metrics: 0.7752485275268555\n",
      "Epoch: 235 Iter 94 Metrics: 0.774482011795044\n",
      "Epoch: 236 Iter 94 Metrics: 0.7737197279930115\n",
      "Epoch: 237 Iter 94 Metrics: 0.7729613184928894\n",
      "Epoch: 238 Iter 94 Metrics: 0.7722064852714539\n",
      "Epoch: 239 Iter 94 Metrics: 0.7714555263519287\n",
      "Epoch: 240 Iter 94 Metrics: 0.770708441734314\n",
      "Epoch: 241 Iter 94 Metrics: 0.7699649333953857\n",
      "Epoch: 242 Iter 94 Metrics: 0.7692253589630127\n",
      "Epoch: 243 Iter 94 Metrics: 0.7684893608093262\n",
      "Epoch: 244 Iter 94 Metrics: 0.7677570581436157\n",
      "Epoch: 245 Iter 94 Metrics: 0.7670286893844604\n",
      "Epoch: 246 Iter 94 Metrics: 0.7663032412528992\n",
      "Epoch: 247 Iter 94 Metrics: 0.7655816674232483\n",
      "Epoch: 248 Iter 94 Metrics: 0.7648638486862183\n",
      "Epoch: 249 Iter 94 Metrics: 0.7641491889953613\n",
      "Epoch: 250 Iter 94 Metrics: 0.7634378671646118\n",
      "Epoch: 251 Iter 94 Metrics: 0.7627305388450623\n",
      "Epoch: 252 Iter 94 Metrics: 0.7620261907577515\n",
      "Epoch: 253 Iter 94 Metrics: 0.7613252401351929\n",
      "Epoch: 254 Iter 94 Metrics: 0.760627806186676\n",
      "Epoch: 255 Iter 94 Metrics: 0.7599336504936218\n",
      "Epoch: 256 Iter 94 Metrics: 0.759242832660675\n",
      "Epoch: 257 Iter 94 Metrics: 0.7585554122924805\n",
      "Epoch: 258 Iter 94 Metrics: 0.7578713893890381\n",
      "Epoch: 259 Iter 94 Metrics: 0.7571902275085449\n",
      "Epoch: 260 Iter 94 Metrics: 0.7565122246742249\n",
      "Epoch: 261 Iter 94 Metrics: 0.7558377385139465\n",
      "Epoch: 262 Iter 94 Metrics: 0.7551663517951965\n",
      "Epoch: 263 Iter 94 Metrics: 0.7544980645179749\n",
      "Epoch: 264 Iter 94 Metrics: 0.7538329362869263\n",
      "Epoch: 265 Iter 94 Metrics: 0.7531709671020508\n",
      "Epoch: 266 Iter 94 Metrics: 0.7525120973587036\n",
      "Epoch: 267 Iter 94 Metrics: 0.7518564462661743\n",
      "Epoch: 268 Iter 94 Metrics: 0.7512037754058838\n",
      "Epoch: 269 Iter 94 Metrics: 0.7505538463592529\n",
      "Epoch: 270 Iter 94 Metrics: 0.7499068975448608\n",
      "Epoch: 271 Iter 94 Metrics: 0.7492631673812866\n",
      "Epoch: 272 Iter 94 Metrics: 0.7486222386360168\n",
      "Epoch: 273 Iter 94 Metrics: 0.7479842901229858\n",
      "Epoch: 274 Iter 94 Metrics: 0.747349202632904\n",
      "Epoch: 275 Iter 94 Metrics: 0.7467169761657715\n",
      "Epoch: 276 Iter 94 Metrics: 0.7460876703262329\n",
      "Epoch: 277 Iter 94 Metrics: 0.7454613447189331\n",
      "Epoch: 278 Iter 94 Metrics: 0.7448375225067139\n",
      "Epoch: 279 Iter 94 Metrics: 0.7442166209220886\n",
      "Epoch: 280 Iter 94 Metrics: 0.7435985803604126\n",
      "Epoch: 281 Iter 94 Metrics: 0.7429832220077515\n",
      "Epoch: 282 Iter 94 Metrics: 0.7423703074455261\n",
      "Epoch: 283 Iter 94 Metrics: 0.7417604327201843\n",
      "Epoch: 284 Iter 94 Metrics: 0.7411531209945679\n",
      "Epoch: 285 Iter 94 Metrics: 0.7405485510826111\n",
      "Epoch: 286 Iter 94 Metrics: 0.7399464249610901\n",
      "Epoch: 287 Iter 94 Metrics: 0.7393470406532288\n",
      "Epoch: 288 Iter 94 Metrics: 0.7387506365776062\n",
      "Epoch: 289 Iter 94 Metrics: 0.7381568551063538\n",
      "Epoch: 290 Iter 94 Metrics: 0.7375651001930237\n",
      "Epoch: 291 Iter 94 Metrics: 0.7369760870933533\n",
      "Epoch: 292 Iter 94 Metrics: 0.7363895177841187\n",
      "Epoch: 293 Iter 94 Metrics: 0.7358059883117676\n",
      "Epoch: 294 Iter 94 Metrics: 0.735224723815918\n",
      "Epoch: 295 Iter 94 Metrics: 0.734645664691925\n",
      "Epoch: 296 Iter 94 Metrics: 0.7340690493583679\n",
      "Epoch: 297 Iter 94 Metrics: 0.7334950566291809\n",
      "Epoch: 298 Iter 94 Metrics: 0.732923686504364\n",
      "Epoch: 299 Iter 94 Metrics: 0.7323545813560486\n"
     ]
    }
   ],
   "source": [
    "epochs = 300\n",
    "for ep in range(epochs):\n",
    "    ep_loss = []\n",
    "    for i, batch in enumerate(iter(train_dl)):\n",
    "        batch = jnp.array(batch)\n",
    "        train_state = train_step(train_state, batch)\n",
    "        train_state = compute_metrics(state=train_state, batch=batch)\n",
    "        metrics = train_state.metrics.compute()['loss']\n",
    "        ep_loss.append(metrics)\n",
    "    \n",
    "    print(f'Epoch: {ep} Iter {i} Metrics: {jnp.mean(np.array(ep_loss))}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 299 Iter 94 Metrics: 0.7318029403686523\n"
     ]
    }
   ],
   "source": [
    "\n",
    "ep_loss = []\n",
    "for i, batch in enumerate(iter(test_dl)):\n",
    "    batch = jnp.array(batch)\n",
    "    train_state = train_step(train_state, batch)\n",
    "    train_state = compute_metrics(state=train_state, batch=batch)\n",
    "    metrics = train_state.metrics.compute()['loss']\n",
    "    ep_loss.append(metrics)\n",
    "\n",
    "print(f'Epoch: {ep} Iter {i} Metrics: {jnp.mean(np.array(ep_loss))}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "R@50: 0.07524786442144529 HR@10: 0.27906976744186046\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def get_HR_jax(state, train_loader, test_loader, top_n=10):\n",
    "\n",
    "    counter, hits = 0, 0\n",
    "    r = []\n",
    "    for i, (batch_train, batch_val) in enumerate(zip(iter(train_loader), iter(test_loader))):\n",
    "        batch_train, batch_val = jnp.array(batch_train), jnp.array(batch_val)\n",
    "        delta = batch_val - batch_train\n",
    "        x_rec = state.apply_fn({'params': state.params}, batch_train)\n",
    "        \n",
    "        for k in range(delta.shape[0]):\n",
    "            non_zero_ind = np.nonzero(np.array(delta[k]))[0]\n",
    "            if len(non_zero_ind) > 0:\n",
    "                counter += 1\n",
    "                diff = set([int(item) for item  in np.argpartition(np.array(x_rec[k]).reshape(-1), -top_n)[-top_n:]]) - (set([int(item) for item in non_zero_ind]))\n",
    "                \n",
    "                if len(diff) < top_n:\n",
    "                    hits += 1\n",
    "    return hits / counter\n",
    "\n",
    "def get_recall_jax(state, train_loader, test_loader, top_n=50):\n",
    "\n",
    "    counter, hits = 0, 0\n",
    "    r = []\n",
    "    for i, (batch_train, batch_val) in enumerate(zip(iter(train_loader), iter(test_loader))):\n",
    "        batch_train, batch_val = jnp.array(batch_train), jnp.array(batch_val)\n",
    "        delta = batch_val - batch_train\n",
    "        \n",
    "        x_rec = state.apply_fn({'params': state.params}, batch_train)\n",
    "        \n",
    "        for k in range(delta.shape[0]):\n",
    "            non_zero_ind = np.nonzero(np.array(delta[k]))[0]\n",
    "            if len(non_zero_ind) > 0:\n",
    "                counter += 1\n",
    "                diff = set([int(item) for item in  np.argpartition(np.array(x_rec[k]).reshape(-1), -top_n)[-top_n:]]).intersection(set([int(item) for item in non_zero_ind]))\n",
    "                r.append(len(diff) / len(non_zero_ind))\n",
    "                if len(diff) < top_n:\n",
    "                    hits += 1\n",
    "    return np.mean(r)\n",
    "        \n",
    "get_recall_jax(train_state, train_dl, test_dl)\n",
    "\n",
    "print(f'R@50: {get_recall_jax(train_state, train_dl, test_dl, top_n=50)} HR@10: {get_HR_jax(train_state, train_dl, test_dl, top_n=10)}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyperbolic AE (Unfortunately it is not working)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "in_dim, out_dim = 3701, 3701\n",
    "\n",
    "hidden_dim = 256\n",
    "\n",
    "dims = [in_dim, hidden_dim, out_dim]\n",
    "c = -1\n",
    "\n",
    "class PoincareSimpleAE(nn.Module):\n",
    "    \n",
    "    param_dtype = jnp.float32\n",
    "    kernel_init: Callable = default_kernel_init\n",
    "    bias_init: Callable = nn.initializers.uniform(scale= 1 / jnp.sqrt(in_dim))\n",
    "        \n",
    "    in_dim, out_dim = in_dim, out_dim\n",
    "    hidden_dim = hidden_dim\n",
    "    c = c\n",
    "    act = nn.relu\n",
    "    \n",
    "    def setup(self):\n",
    "        self.scalars_1 = self.param(\n",
    "            \"scalars@1\",\n",
    "            self.kernel_init,\n",
    "            (self.hidden_dim, self.in_dim),\n",
    "            self.param_dtype,\n",
    "        )\n",
    "        \n",
    "        self.bias_poincare_1 = self.param(\n",
    "            \"bias_poincare@1\",\n",
    "            self.bias_init,\n",
    "            (self.hidden_dim,),\n",
    "            self.param_dtype,\n",
    "        )\n",
    "        \n",
    "        self.scalars_2 = self.param(\n",
    "            \"scalars@2\",\n",
    "            self.kernel_init,\n",
    "            (self.out_dim, self.hidden_dim),\n",
    "            self.param_dtype,\n",
    "        )\n",
    "    \n",
    "        self.bias_poincare_2 = self.param(\n",
    "            \"bias_poincare@2\",\n",
    "            self.bias_init,\n",
    "            (self.out_dim,),\n",
    "            self.param_dtype,\n",
    "        )\n",
    "        \n",
    "        self.balls = {'bias_poincare@1': PoincareBall(self.hidden_dim, self.c),\n",
    "                      'bias_poincare@2': PoincareBall(self.out_dim, self.c)}\n",
    "        \n",
    "        ball_in = PoincareBall(self.in_dim, self.c)\n",
    "        self.matvec_1 = jax.vmap(ball_in.mobius_matvec, in_axes=(None, 0), out_axes=0)\n",
    "        \n",
    "        ball_hidden = PoincareBall(self.hidden_dim, self.c)\n",
    "        self.add_1 = jax.vmap(ball_hidden.mobius_add, in_axes=(None, 0), out_axes=0)\n",
    "        self.log_hidden = jax.vmap(ball_hidden.log, in_axes=(None, 0), out_axes=0)\n",
    "        self.exp_hidden = jax.vmap(ball_hidden.exp, in_axes=(None, 0), out_axes=0)\n",
    "        \n",
    "        self.matvec_2 = jax.vmap(ball_hidden.mobius_matvec, in_axes=(None, 0), out_axes=0)\n",
    "        ball_out = PoincareBall(self.out_dim, self.c)\n",
    "        self.add_2 = jax.vmap(ball_out.mobius_add, in_axes=(None, 0), out_axes=0)\n",
    "\n",
    "    def __call__(self, x):\n",
    "        \n",
    "        Ax = self.matvec_1(self.scalars_1, x)\n",
    "        Ax_b = self.add_1(self.bias_poincare_1, Ax)\n",
    "        \n",
    "        activation_hid = self.log_hidden(jnp.zeros([Ax_b.shape[1]]), Ax_b)\n",
    "        activation_hid = nn.relu(activation_hid)\n",
    "        activation_hid = self.exp_hidden(jnp.zeros([Ax_b.shape[1]]), Ax_b)\n",
    "\n",
    "        output = self.matvec_2(self.scalars_2, activation_hid)\n",
    "        output = self.add_2(self.bias_poincare_2, output)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from rieoptax.geometry.hyperbolic import PoincareBall\n",
    "\n",
    "from flax.training import train_state\n",
    "class TrainStateRiemannianMLP(train_state.TrainState):\n",
    "    metrics: Metrics\n",
    "    balls = {'bias_poincare@1': PoincareBall(hidden_dim, c),\n",
    "                  'bias_poincare@2': PoincareBall(out_dim, c)}\n",
    "    \n",
    "    def apply_gradients(self, *, grads, **kwargs):\n",
    "        \"\"\"Updates `step`, `params`, `opt_state` and `**kwargs` in return value.\n",
    "\n",
    "        Note that internally this function calls `.tx.update()` followed by a call\n",
    "        to `optax.apply_updates()` to update `params` and `opt_state`.\n",
    "\n",
    "        Args:\n",
    "          grads: Gradients that have the same pytree structure as `.params`.\n",
    "          **kwargs: Additional dataclass attributes that should be `.replace()`-ed.\n",
    "\n",
    "        Returns:\n",
    "          An updated instance of `self` with `step` incremented by one, `params`\n",
    "          and `opt_state` updated by applying `grads`, and additional attributes\n",
    "          replaced as specified by `kwargs`.\n",
    "        \"\"\"\n",
    "        updates, new_opt_state = self.tx.update(\n",
    "            grads, self.opt_state, self.params)\n",
    "        \n",
    "        \n",
    "        poincare_param_names = self.balls.keys()\n",
    "        r_grad_poincare_bias, old_bias_params = {}, {}\n",
    "        for name in poincare_param_names:\n",
    "            \n",
    "            old_bias_params[name] = self.params[name]\n",
    "            bias_poincare_grads = updates[name]\n",
    "            r_grad_poincare_bias[name] = self.balls[name].egrad_to_rgrad(old_bias_params[name], bias_poincare_grads)\n",
    "        \n",
    "        updates = updates.unfreeze()\n",
    "        for name in poincare_param_names:\n",
    "            updates[name] = r_grad_poincare_bias[name]\n",
    "            \n",
    "        updates = flax.core.frozen_dict.freeze(updates)\n",
    "        \n",
    "        new_params = optax.apply_updates(self.params, updates)\n",
    "        \n",
    "        lr = self.lr\n",
    "        new_params = new_params.unfreeze()\n",
    "        for name in poincare_param_names:\n",
    "            bias_poincare_new = new_params[name]\n",
    "            tv = lr * r_grad_poincare_bias[name]\n",
    "            new_params[name] = self.balls[name].exp(old_bias_params[name], tv)\n",
    "        \n",
    "        new_params = flax.core.frozen_dict.freeze(new_params)\n",
    "        \n",
    "        return self.replace(\n",
    "            step=self.step + 1,\n",
    "            params=new_params,\n",
    "            opt_state=new_opt_state,\n",
    "            **kwargs,\n",
    "        )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def create_train_state(module, rng, learning_rate):\n",
    "  \"\"\"Creates an initial `TrainState`.\"\"\"\n",
    "  from flax.training import train_state\n",
    "  class TrainState(TrainStateRiemannianMLP):\n",
    "    metrics: Metrics\n",
    "    lr = learning_rate\n",
    "  params = module.init(rng, jnp.ones([batch_size, in_dim]))['params'] # initialize parameters by passing a template image\n",
    "  tx = optax.sgd(learning_rate)\n",
    "  return TrainState.create(\n",
    "      apply_fn=module.apply, params=params, tx=tx,\n",
    "      metrics=Metrics.empty())\n",
    "\n",
    "\n",
    "def MSEMasked(inputs, targets): # Masking into a vector of 1's and 0's.\n",
    "    mask = (targets!=0)\n",
    "    mask = jnp.float32(mask)\n",
    "\n",
    "    # Actual number of ratings.\n",
    "    # Take max to avoid division by zero while calculating loss.\n",
    "    other = jnp.array(1.0)\n",
    "    number_ratings = jnp.maximum(jnp.sum(mask), other)\n",
    "    error = jnp.sum(mask * (targets - inputs) ** 2)\n",
    "    loss = error / number_ratings\n",
    "    return loss\n",
    "\n",
    "@jax.jit\n",
    "def train_step(state, batch):\n",
    "  \"\"\"Train for a single step.\"\"\"\n",
    "\n",
    "  def loss_fn(params):\n",
    "    x_rec = state.apply_fn({'params': params}, batch)\n",
    "    loss = MSEMasked(batch, x_rec)\n",
    "    return loss\n",
    "  \n",
    "  grad_fn = jax.grad(loss_fn)\n",
    "  grads = grad_fn(state.params)\n",
    "  state = state.apply_gradients(grads=grads)\n",
    "  return state\n",
    "\n",
    "@jax.jit\n",
    "def compute_metrics(*, state, batch):\n",
    "    x_rec = state.apply_fn({'params': state.params}, batch)\n",
    "    loss = MSEMasked(batch, x_rec)\n",
    "    metric_updates = state.metrics.single_from_model_output(loss=loss)\n",
    "    metrics = state.metrics.merge(metric_updates)\n",
    "    state = state.replace(metrics=metrics)\n",
    "    return state\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "poincare_ae = PoincareSimpleAE()\n",
    "train_state = create_train_state(poincare_ae, jax.random.PRNGKey(42), 1e-03)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0 Iter 94 Metrics: 0.0004129254666622728\n",
      "Epoch: 1 Iter 94 Metrics: 0.00041290681110695004\n",
      "Epoch: 2 Iter 94 Metrics: 0.00041289953514933586\n",
      "Epoch: 3 Iter 94 Metrics: 0.00041289563523605466\n",
      "Epoch: 4 Iter 94 Metrics: 0.0004128931905142963\n",
      "Epoch: 5 Iter 94 Metrics: 0.0004128915898036212\n",
      "Epoch: 6 Iter 94 Metrics: 0.00041289033833891153\n",
      "Epoch: 7 Iter 94 Metrics: 0.00041288946522399783\n",
      "Epoch: 8 Iter 94 Metrics: 0.00041288885404355824\n",
      "Epoch: 9 Iter 94 Metrics: 0.00041288833017461\n",
      "Epoch: 10 Iter 94 Metrics: 0.00041288789361715317\n",
      "Epoch: 11 Iter 94 Metrics: 0.00041288757347501814\n",
      "Epoch: 12 Iter 94 Metrics: 0.00041288742795586586\n",
      "Epoch: 13 Iter 94 Metrics: 0.0004128873406443745\n",
      "Epoch: 14 Iter 94 Metrics: 0.0004128873406443745\n",
      "Epoch: 15 Iter 94 Metrics: 0.0004128873406443745\n",
      "Epoch: 16 Iter 94 Metrics: 0.0004128872824367136\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[67], line 9\u001b[0m\n\u001b[1;32m      7\u001b[0m         batch \u001b[38;5;241m=\u001b[39m jnp\u001b[38;5;241m.\u001b[39mdivide(batch, jnp\u001b[38;5;241m.\u001b[39mexpand_dims(jnp\u001b[38;5;241m.\u001b[39mlinalg\u001b[38;5;241m.\u001b[39mnorm(batch, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m), axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m) \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1e-5\u001b[39m)\n\u001b[1;32m      8\u001b[0m \u001b[38;5;66;03m#         print(jnp.linalg.norm(batch, axis=1))\u001b[39;00m\n\u001b[0;32m----> 9\u001b[0m         train_state \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_state\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     10\u001b[0m         train_state \u001b[38;5;241m=\u001b[39m compute_metrics(state\u001b[38;5;241m=\u001b[39mtrain_state, batch\u001b[38;5;241m=\u001b[39mbatch)\n\u001b[1;32m     11\u001b[0m         metrics \u001b[38;5;241m=\u001b[39m train_state\u001b[38;5;241m.\u001b[39mmetrics\u001b[38;5;241m.\u001b[39mcompute()[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mloss\u001b[39m\u001b[38;5;124m'\u001b[39m]\n",
      "    \u001b[0;31m[... skipping hidden 1 frame]\u001b[0m\n",
      "File \u001b[0;32m~/projects/python_venv/lib/python3.10/site-packages/jax/_src/pjit.py:235\u001b[0m, in \u001b[0;36m_cpp_pjit.<locals>.cache_miss\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    233\u001b[0m \u001b[38;5;129m@api_boundary\u001b[39m\n\u001b[1;32m    234\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcache_miss\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m--> 235\u001b[0m   outs, out_flat, out_tree, args_flat \u001b[38;5;241m=\u001b[39m \u001b[43m_python_pjit_helper\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    236\u001b[0m \u001b[43m      \u001b[49m\u001b[43mfun\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minfer_params_fn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    238\u001b[0m   executable \u001b[38;5;241m=\u001b[39m _read_most_recent_pjit_call_executable()\n\u001b[1;32m    240\u001b[0m   use_fastpath \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m    241\u001b[0m       executable \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m\n\u001b[1;32m    242\u001b[0m       \u001b[38;5;28misinstance\u001b[39m(executable, pxla\u001b[38;5;241m.\u001b[39mMeshExecutable) \u001b[38;5;129;01mand\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    248\u001b[0m       \u001b[38;5;28mall\u001b[39m(\u001b[38;5;28misinstance\u001b[39m(x, xc\u001b[38;5;241m.\u001b[39mArrayImpl) \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m out_flat)\n\u001b[1;32m    249\u001b[0m   )\n",
      "File \u001b[0;32m~/projects/python_venv/lib/python3.10/site-packages/jax/_src/pjit.py:184\u001b[0m, in \u001b[0;36m_python_pjit_helper\u001b[0;34m(fun, infer_params_fn, *args, **kwargs)\u001b[0m\n\u001b[1;32m    182\u001b[0m   dispatch\u001b[38;5;241m.\u001b[39mcheck_arg(arg)\n\u001b[1;32m    183\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 184\u001b[0m   out_flat \u001b[38;5;241m=\u001b[39m \u001b[43mpjit_p\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbind\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs_flat\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mparams\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    185\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m pxla\u001b[38;5;241m.\u001b[39mDeviceAssignmentMismatchError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    186\u001b[0m   fails, \u001b[38;5;241m=\u001b[39m e\u001b[38;5;241m.\u001b[39margs\n",
      "File \u001b[0;32m~/projects/python_venv/lib/python3.10/site-packages/jax/_src/core.py:2577\u001b[0m, in \u001b[0;36mAxisPrimitive.bind\u001b[0;34m(self, *args, **params)\u001b[0m\n\u001b[1;32m   2573\u001b[0m axis_main \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mmax\u001b[39m((axis_frame(a)\u001b[38;5;241m.\u001b[39mmain_trace \u001b[38;5;28;01mfor\u001b[39;00m a \u001b[38;5;129;01min\u001b[39;00m used_axis_names(\u001b[38;5;28mself\u001b[39m, params)),\n\u001b[1;32m   2574\u001b[0m                 default\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, key\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mlambda\u001b[39;00m t: \u001b[38;5;28mgetattr\u001b[39m(t, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlevel\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m))\n\u001b[1;32m   2575\u001b[0m top_trace \u001b[38;5;241m=\u001b[39m (top_trace \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m axis_main \u001b[38;5;129;01mor\u001b[39;00m axis_main\u001b[38;5;241m.\u001b[39mlevel \u001b[38;5;241m<\u001b[39m top_trace\u001b[38;5;241m.\u001b[39mlevel\n\u001b[1;32m   2576\u001b[0m              \u001b[38;5;28;01melse\u001b[39;00m axis_main\u001b[38;5;241m.\u001b[39mwith_cur_sublevel())\n\u001b[0;32m-> 2577\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbind_with_trace\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtop_trace\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparams\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/projects/python_venv/lib/python3.10/site-packages/jax/_src/core.py:363\u001b[0m, in \u001b[0;36mPrimitive.bind_with_trace\u001b[0;34m(self, trace, args, params)\u001b[0m\n\u001b[1;32m    362\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mbind_with_trace\u001b[39m(\u001b[38;5;28mself\u001b[39m, trace, args, params):\n\u001b[0;32m--> 363\u001b[0m   out \u001b[38;5;241m=\u001b[39m \u001b[43mtrace\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mprocess_primitive\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mmap\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mtrace\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfull_raise\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparams\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    364\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mmap\u001b[39m(full_lower, out) \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmultiple_results \u001b[38;5;28;01melse\u001b[39;00m full_lower(out)\n",
      "File \u001b[0;32m~/projects/python_venv/lib/python3.10/site-packages/jax/_src/core.py:807\u001b[0m, in \u001b[0;36mEvalTrace.process_primitive\u001b[0;34m(self, primitive, tracers, params)\u001b[0m\n\u001b[1;32m    806\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mprocess_primitive\u001b[39m(\u001b[38;5;28mself\u001b[39m, primitive, tracers, params):\n\u001b[0;32m--> 807\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mprimitive\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mimpl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mtracers\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mparams\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/projects/python_venv/lib/python3.10/site-packages/jax/_src/pjit.py:1291\u001b[0m, in \u001b[0;36m_pjit_call_impl\u001b[0;34m(jaxpr, in_shardings, out_shardings, resource_env, donated_invars, name, in_positional_semantics, out_positional_semantics, keep_unused, inline, *args)\u001b[0m\n\u001b[1;32m   1289\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1290\u001b[0m   _allow_propagation_to_outputs \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28;01mFalse\u001b[39;00m] \u001b[38;5;241m*\u001b[39m \u001b[38;5;28mlen\u001b[39m(out_shardings)\n\u001b[0;32m-> 1291\u001b[0m compiled \u001b[38;5;241m=\u001b[39m \u001b[43m_pjit_lower\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1292\u001b[0m \u001b[43m    \u001b[49m\u001b[43mjaxpr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43min_shardings\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mout_shardings\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mresource_env\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1293\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdonated_invars\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43min_is_global\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkeep_unused\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1294\u001b[0m \u001b[43m    \u001b[49m\u001b[43malways_lower\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlowering_platform\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mcompile(\n\u001b[1;32m   1295\u001b[0m         _allow_propagation_to_outputs\u001b[38;5;241m=\u001b[39m_allow_propagation_to_outputs)\n\u001b[1;32m   1296\u001b[0m _most_recent_pjit_call_executable\u001b[38;5;241m.\u001b[39mvalue \u001b[38;5;241m=\u001b[39m compiled\n\u001b[1;32m   1297\u001b[0m \u001b[38;5;66;03m# This check is expensive so only do it if enable_checks is on.\u001b[39;00m\n",
      "File \u001b[0;32m~/projects/python_venv/lib/python3.10/site-packages/jax/_src/pjit.py:1377\u001b[0m, in \u001b[0;36m_pjit_lower\u001b[0;34m(jaxpr, in_shardings, out_shardings, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1375\u001b[0m in_shardings \u001b[38;5;241m=\u001b[39m SameDeviceAssignmentTuple(in_shardings, da)\n\u001b[1;32m   1376\u001b[0m out_shardings \u001b[38;5;241m=\u001b[39m SameDeviceAssignmentTuple(out_shardings, da)\n\u001b[0;32m-> 1377\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_pjit_lower_cached\u001b[49m\u001b[43m(\u001b[49m\u001b[43mjaxpr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43min_shardings\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mout_shardings\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/projects/python_venv/lib/python3.10/site-packages/jax/_src/pjit.py:1437\u001b[0m, in \u001b[0;36m_pjit_lower_cached\u001b[0;34m(jaxpr, sdat_in_shardings, sdat_out_shardings, resource_env, donated_invars, name, in_is_global, keep_unused, always_lower, lowering_platform)\u001b[0m\n\u001b[1;32m   1431\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m pxla\u001b[38;5;241m.\u001b[39mlower_mesh_computation(\n\u001b[1;32m   1432\u001b[0m     jaxpr, api_name, name, mesh,\n\u001b[1;32m   1433\u001b[0m     in_shardings, out_shardings, donated_invars,\n\u001b[1;32m   1434\u001b[0m     \u001b[38;5;28;01mTrue\u001b[39;00m, jaxpr\u001b[38;5;241m.\u001b[39min_avals, tiling_method\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, in_is_global\u001b[38;5;241m=\u001b[39min_is_global,\n\u001b[1;32m   1435\u001b[0m     lowering_platform\u001b[38;5;241m=\u001b[39mlowering_platform)\n\u001b[1;32m   1436\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1437\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mpxla\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlower_sharding_computation\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1438\u001b[0m \u001b[43m      \u001b[49m\u001b[43mjaxpr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mapi_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43min_shardings\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mout_shardings\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdonated_invars\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1439\u001b[0m \u001b[43m      \u001b[49m\u001b[43mjaxpr\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43min_avals\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43min_is_global\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43min_is_global\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkeep_unused\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkeep_unused\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1440\u001b[0m \u001b[43m      \u001b[49m\u001b[43malways_lower\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43malways_lower\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1441\u001b[0m \u001b[43m      \u001b[49m\u001b[43mdevices_from_context\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1442\u001b[0m \u001b[43m          \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mmesh\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mis\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mmesh\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mempty\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mmesh\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdevices\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mflat\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1443\u001b[0m \u001b[43m      \u001b[49m\u001b[43mlowering_platform\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlowering_platform\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/projects/python_venv/lib/python3.10/site-packages/jax/_src/profiler.py:314\u001b[0m, in \u001b[0;36mannotate_function.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    311\u001b[0m \u001b[38;5;129m@wraps\u001b[39m(func)\n\u001b[1;32m    312\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mwrapper\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    313\u001b[0m   \u001b[38;5;28;01mwith\u001b[39;00m TraceAnnotation(name, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mdecorator_kwargs):\n\u001b[0;32m--> 314\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    315\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m wrapper\n",
      "File \u001b[0;32m~/projects/python_venv/lib/python3.10/site-packages/jax/_src/interpreters/pxla.py:3082\u001b[0m, in \u001b[0;36mlower_sharding_computation\u001b[0;34m(fun_or_jaxpr, api_name, fun_name, in_shardings, out_shardings, donated_invars, global_in_avals, in_is_global, keep_unused, always_lower, devices_from_context, lowering_platform)\u001b[0m\n\u001b[1;32m   3080\u001b[0m ordered_effects \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(effects\u001b[38;5;241m.\u001b[39mordered_effects\u001b[38;5;241m.\u001b[39mfilter_in(closed_jaxpr\u001b[38;5;241m.\u001b[39meffects))\n\u001b[1;32m   3081\u001b[0m arg_names, result_names \u001b[38;5;241m=\u001b[39m _debug_names(jaxpr\u001b[38;5;241m.\u001b[39mdebug_info, kept_var_idx)\n\u001b[0;32m-> 3082\u001b[0m lowering_result \u001b[38;5;241m=\u001b[39m \u001b[43mmlir\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlower_jaxpr_to_module\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   3083\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodule_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3084\u001b[0m \u001b[43m    \u001b[49m\u001b[43mclosed_jaxpr\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3085\u001b[0m \u001b[43m    \u001b[49m\u001b[43munordered_effects\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3086\u001b[0m \u001b[43m    \u001b[49m\u001b[43mordered_effects\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3087\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbackend\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3088\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# Optionally, override the lowering platform\u001b[39;49;00m\n\u001b[1;32m   3089\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlowering_platform\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mbackend\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mplatform\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3090\u001b[0m \u001b[43m    \u001b[49m\u001b[43maxis_ctx\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3091\u001b[0m \u001b[43m    \u001b[49m\u001b[43mname_stack\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3092\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdonated_invars\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3093\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreplicated_args\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreplicated_args\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3094\u001b[0m \u001b[43m    \u001b[49m\u001b[43marg_shardings\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43min_op_shardings\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3095\u001b[0m \u001b[43m    \u001b[49m\u001b[43mresult_shardings\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mout_op_shardings\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3096\u001b[0m \u001b[43m    \u001b[49m\u001b[43marg_names\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43marg_names\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mresult_names\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresult_names\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3098\u001b[0m module, keepalive, host_callbacks \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m   3099\u001b[0m     lowering_result\u001b[38;5;241m.\u001b[39mmodule, lowering_result\u001b[38;5;241m.\u001b[39mkeepalive,\n\u001b[1;32m   3100\u001b[0m     lowering_result\u001b[38;5;241m.\u001b[39mhost_callbacks)\n\u001b[1;32m   3102\u001b[0m \u001b[38;5;66;03m# backend and device_assignment is passed through to MeshExecutable because\u001b[39;00m\n\u001b[1;32m   3103\u001b[0m \u001b[38;5;66;03m# if keep_unused=False and all in_shardings are pruned, then there is no way\u001b[39;00m\n\u001b[1;32m   3104\u001b[0m \u001b[38;5;66;03m# to get the device_assignment and backend. So pass it to MeshExecutable\u001b[39;00m\n\u001b[1;32m   3105\u001b[0m \u001b[38;5;66;03m# because we calculate the device_assignment and backend before in_shardings,\u001b[39;00m\n\u001b[1;32m   3106\u001b[0m \u001b[38;5;66;03m# etc are pruned.\u001b[39;00m\n",
      "File \u001b[0;32m~/projects/python_venv/lib/python3.10/site-packages/jax/_src/interpreters/mlir.py:742\u001b[0m, in \u001b[0;36mlower_jaxpr_to_module\u001b[0;34m(module_name, jaxpr, unordered_effects, ordered_effects, backend_or_name, platform, axis_context, name_stack, donated_args, replicated_args, arg_shardings, result_shardings, arg_names, result_names)\u001b[0m\n\u001b[1;32m    739\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m unlowerable_effects:\n\u001b[1;32m    740\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    741\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mCannot lower jaxpr with unlowerable effects: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00munlowerable_effects\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m--> 742\u001b[0m   \u001b[43mlower_jaxpr_to_fun\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    743\u001b[0m \u001b[43m      \u001b[49m\u001b[43mctx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmain\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mjaxpr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mordered_effects\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpublic\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_tokens\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    744\u001b[0m \u001b[43m      \u001b[49m\u001b[43mreplace_tokens_with_dummy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    745\u001b[0m \u001b[43m      \u001b[49m\u001b[43mnum_output_tokens\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    746\u001b[0m \u001b[43m      \u001b[49m\u001b[43mreplicated_args\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreplicated_args\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    747\u001b[0m \u001b[43m      \u001b[49m\u001b[43marg_shardings\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43marg_shardings\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mresult_shardings\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresult_shardings\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    748\u001b[0m \u001b[43m      \u001b[49m\u001b[43minput_output_aliases\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minput_output_aliases\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    749\u001b[0m \u001b[43m      \u001b[49m\u001b[43marg_names\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43marg_names\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mresult_names\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresult_names\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    751\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m ctx\u001b[38;5;241m.\u001b[39mmodule\u001b[38;5;241m.\u001b[39moperation\u001b[38;5;241m.\u001b[39mverify():\n\u001b[1;32m    752\u001b[0m   module_string \u001b[38;5;241m=\u001b[39m module_to_string(ctx\u001b[38;5;241m.\u001b[39mmodule)\n",
      "File \u001b[0;32m~/projects/python_venv/lib/python3.10/site-packages/jax/_src/interpreters/mlir.py:1044\u001b[0m, in \u001b[0;36mlower_jaxpr_to_fun\u001b[0;34m(ctx, name, jaxpr, effects, create_tokens, public, replace_tokens_with_dummy, replicated_args, arg_shardings, result_shardings, use_sharding_annotations, input_output_aliases, num_output_tokens, api_name, arg_names, result_names)\u001b[0m\n\u001b[1;32m   1042\u001b[0m     args\u001b[38;5;241m.\u001b[39mappend(arg)\n\u001b[1;32m   1043\u001b[0m callee_name_stack \u001b[38;5;241m=\u001b[39m ctx\u001b[38;5;241m.\u001b[39mname_stack\u001b[38;5;241m.\u001b[39mextend(util\u001b[38;5;241m.\u001b[39mwrap_name(name, api_name))\n\u001b[0;32m-> 1044\u001b[0m out_vals, tokens_out \u001b[38;5;241m=\u001b[39m \u001b[43mjaxpr_subcomp\u001b[49m\u001b[43m(\u001b[49m\u001b[43mctx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreplace\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname_stack\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcallee_name_stack\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1045\u001b[0m \u001b[43m                                     \u001b[49m\u001b[43mjaxpr\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mjaxpr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtokens_in\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mmap\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mir_constants\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mjaxpr\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconsts\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1046\u001b[0m \u001b[43m                                     \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdim_var_values\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdim_var_values\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1047\u001b[0m outs \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m   1048\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m create_tokens:\n",
      "File \u001b[0;32m~/projects/python_venv/lib/python3.10/site-packages/jax/_src/interpreters/mlir.py:1179\u001b[0m, in \u001b[0;36mjaxpr_subcomp\u001b[0;34m(ctx, jaxpr, tokens, consts, dim_var_values, *args)\u001b[0m\n\u001b[1;32m   1175\u001b[0m   axis_size_env \u001b[38;5;241m=\u001b[39m {d: read(d)[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m   1176\u001b[0m                    \u001b[38;5;28;01mfor\u001b[39;00m a \u001b[38;5;129;01min\u001b[39;00m avals_in \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mtype\u001b[39m(a) \u001b[38;5;129;01mis\u001b[39;00m core\u001b[38;5;241m.\u001b[39mDShapedArray\n\u001b[1;32m   1177\u001b[0m                    \u001b[38;5;28;01mfor\u001b[39;00m d \u001b[38;5;129;01min\u001b[39;00m a\u001b[38;5;241m.\u001b[39mshape \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mtype\u001b[39m(d) \u001b[38;5;129;01mis\u001b[39;00m core\u001b[38;5;241m.\u001b[39mVar}\n\u001b[1;32m   1178\u001b[0m   rule_ctx \u001b[38;5;241m=\u001b[39m rule_ctx\u001b[38;5;241m.\u001b[39mreplace(axis_size_env\u001b[38;5;241m=\u001b[39maxis_size_env)\n\u001b[0;32m-> 1179\u001b[0m ans \u001b[38;5;241m=\u001b[39m \u001b[43mrule\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrule_ctx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43mmap\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m_unwrap_singleton_ir_values\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43min_nodes\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1180\u001b[0m \u001b[43m           \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43meqn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparams\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1181\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m effects:\n\u001b[1;32m   1182\u001b[0m   \u001b[38;5;66;03m# If there were ordered effects in the primitive, there should be output\u001b[39;00m\n\u001b[1;32m   1183\u001b[0m   \u001b[38;5;66;03m# tokens we need for subsequent ordered effects.\u001b[39;00m\n\u001b[1;32m   1184\u001b[0m   tokens_out \u001b[38;5;241m=\u001b[39m rule_ctx\u001b[38;5;241m.\u001b[39mtokens_out\n",
      "File \u001b[0;32m~/projects/python_venv/lib/python3.10/site-packages/jax/_src/pjit.py:1489\u001b[0m, in \u001b[0;36m_pjit_lowering\u001b[0;34m(ctx, name, jaxpr, in_shardings, out_shardings, resource_env, donated_invars, in_positional_semantics, out_positional_semantics, keep_unused, inline, *args)\u001b[0m\n\u001b[1;32m   1483\u001b[0m result_shardings \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01mif\u001b[39;00m _is_unspecified(o) \u001b[38;5;28;01melse\u001b[39;00m o\u001b[38;5;241m.\u001b[39m_to_xla_op_sharding(aval\u001b[38;5;241m.\u001b[39mndim)\n\u001b[1;32m   1484\u001b[0m                     \u001b[38;5;28;01mfor\u001b[39;00m aval, o \u001b[38;5;129;01min\u001b[39;00m safe_zip(ctx\u001b[38;5;241m.\u001b[39mavals_out, out_shardings)]\n\u001b[1;32m   1486\u001b[0m \u001b[38;5;66;03m# TODO(b/228598865): inlined calls cannot have shardings set directly on the\u001b[39;00m\n\u001b[1;32m   1487\u001b[0m \u001b[38;5;66;03m# inputs or outputs because they are lost during MLIR->HLO conversion.\u001b[39;00m\n\u001b[1;32m   1488\u001b[0m \u001b[38;5;66;03m# using_sharding_annotation=False means we add an identity operation instead.\u001b[39;00m\n\u001b[0;32m-> 1489\u001b[0m func \u001b[38;5;241m=\u001b[39m \u001b[43mmlir\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlower_jaxpr_to_fun\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1490\u001b[0m \u001b[43m    \u001b[49m\u001b[43mctx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodule_context\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mjaxpr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43meffects\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43marg_shardings\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43marg_shardings\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1491\u001b[0m \u001b[43m    \u001b[49m\u001b[43mresult_shardings\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresult_shardings\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43muse_sharding_annotations\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m   1492\u001b[0m \u001b[43m    \u001b[49m\u001b[43mapi_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mjit\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mresource_env\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mis\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mpjit\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1493\u001b[0m tokens_in \u001b[38;5;241m=\u001b[39m [ctx\u001b[38;5;241m.\u001b[39mtokens_in\u001b[38;5;241m.\u001b[39mget(eff) \u001b[38;5;28;01mfor\u001b[39;00m eff \u001b[38;5;129;01min\u001b[39;00m effects]\n\u001b[1;32m   1494\u001b[0m args \u001b[38;5;241m=\u001b[39m (\u001b[38;5;241m*\u001b[39mctx\u001b[38;5;241m.\u001b[39mdim_var_values, \u001b[38;5;241m*\u001b[39mtokens_in, \u001b[38;5;241m*\u001b[39margs)\n",
      "File \u001b[0;32m~/projects/python_venv/lib/python3.10/site-packages/jax/_src/interpreters/mlir.py:1044\u001b[0m, in \u001b[0;36mlower_jaxpr_to_fun\u001b[0;34m(ctx, name, jaxpr, effects, create_tokens, public, replace_tokens_with_dummy, replicated_args, arg_shardings, result_shardings, use_sharding_annotations, input_output_aliases, num_output_tokens, api_name, arg_names, result_names)\u001b[0m\n\u001b[1;32m   1042\u001b[0m     args\u001b[38;5;241m.\u001b[39mappend(arg)\n\u001b[1;32m   1043\u001b[0m callee_name_stack \u001b[38;5;241m=\u001b[39m ctx\u001b[38;5;241m.\u001b[39mname_stack\u001b[38;5;241m.\u001b[39mextend(util\u001b[38;5;241m.\u001b[39mwrap_name(name, api_name))\n\u001b[0;32m-> 1044\u001b[0m out_vals, tokens_out \u001b[38;5;241m=\u001b[39m \u001b[43mjaxpr_subcomp\u001b[49m\u001b[43m(\u001b[49m\u001b[43mctx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreplace\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname_stack\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcallee_name_stack\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1045\u001b[0m \u001b[43m                                     \u001b[49m\u001b[43mjaxpr\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mjaxpr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtokens_in\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mmap\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mir_constants\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mjaxpr\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconsts\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1046\u001b[0m \u001b[43m                                     \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdim_var_values\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdim_var_values\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1047\u001b[0m outs \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m   1048\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m create_tokens:\n",
      "File \u001b[0;32m~/projects/python_venv/lib/python3.10/site-packages/jax/_src/interpreters/mlir.py:1179\u001b[0m, in \u001b[0;36mjaxpr_subcomp\u001b[0;34m(ctx, jaxpr, tokens, consts, dim_var_values, *args)\u001b[0m\n\u001b[1;32m   1175\u001b[0m   axis_size_env \u001b[38;5;241m=\u001b[39m {d: read(d)[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m   1176\u001b[0m                    \u001b[38;5;28;01mfor\u001b[39;00m a \u001b[38;5;129;01min\u001b[39;00m avals_in \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mtype\u001b[39m(a) \u001b[38;5;129;01mis\u001b[39;00m core\u001b[38;5;241m.\u001b[39mDShapedArray\n\u001b[1;32m   1177\u001b[0m                    \u001b[38;5;28;01mfor\u001b[39;00m d \u001b[38;5;129;01min\u001b[39;00m a\u001b[38;5;241m.\u001b[39mshape \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mtype\u001b[39m(d) \u001b[38;5;129;01mis\u001b[39;00m core\u001b[38;5;241m.\u001b[39mVar}\n\u001b[1;32m   1178\u001b[0m   rule_ctx \u001b[38;5;241m=\u001b[39m rule_ctx\u001b[38;5;241m.\u001b[39mreplace(axis_size_env\u001b[38;5;241m=\u001b[39maxis_size_env)\n\u001b[0;32m-> 1179\u001b[0m ans \u001b[38;5;241m=\u001b[39m \u001b[43mrule\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrule_ctx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43mmap\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m_unwrap_singleton_ir_values\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43min_nodes\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1180\u001b[0m \u001b[43m           \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43meqn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparams\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1181\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m effects:\n\u001b[1;32m   1182\u001b[0m   \u001b[38;5;66;03m# If there were ordered effects in the primitive, there should be output\u001b[39;00m\n\u001b[1;32m   1183\u001b[0m   \u001b[38;5;66;03m# tokens we need for subsequent ordered effects.\u001b[39;00m\n\u001b[1;32m   1184\u001b[0m   tokens_out \u001b[38;5;241m=\u001b[39m rule_ctx\u001b[38;5;241m.\u001b[39mtokens_out\n",
      "File \u001b[0;32m~/projects/python_venv/lib/python3.10/site-packages/jax/_src/lax/lax.py:3837\u001b[0m, in \u001b[0;36m_unary_reduce_lower\u001b[0;34m(reducer, unit_factory, ctx, x, axes)\u001b[0m\n\u001b[1;32m   3835\u001b[0m aval_out, \u001b[38;5;241m=\u001b[39m ctx\u001b[38;5;241m.\u001b[39mavals_out\n\u001b[1;32m   3836\u001b[0m dtype \u001b[38;5;241m=\u001b[39m aval_out\u001b[38;5;241m.\u001b[39mdtype\n\u001b[0;32m-> 3837\u001b[0m op \u001b[38;5;241m=\u001b[39m \u001b[43mhlo\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mReduceOp\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[43mmlir\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43maval_to_ir_type\u001b[49m\u001b[43m(\u001b[49m\u001b[43maval_out\u001b[49m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\u001b[43mx\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3838\u001b[0m \u001b[43m                  \u001b[49m\u001b[43mmlir\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mir_constants\u001b[49m\u001b[43m(\u001b[49m\u001b[43munit_factory\u001b[49m\u001b[43m(\u001b[49m\u001b[43maval_out\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3839\u001b[0m \u001b[43m                  \u001b[49m\u001b[43mmlir\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdense_int_elements\u001b[49m\u001b[43m(\u001b[49m\u001b[43maxes\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3840\u001b[0m scalar_type \u001b[38;5;241m=\u001b[39m mlir\u001b[38;5;241m.\u001b[39maval_to_ir_type(core\u001b[38;5;241m.\u001b[39mShapedArray((), dtype))\n\u001b[1;32m   3841\u001b[0m reducer_region \u001b[38;5;241m=\u001b[39m op\u001b[38;5;241m.\u001b[39mregions[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mblocks\u001b[38;5;241m.\u001b[39mappend(scalar_type, scalar_type)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "\n",
    "epochs = 300\n",
    "for ep in range(epochs):\n",
    "    ep_loss = []\n",
    "    for i, batch in enumerate(iter(train_dl)):\n",
    "        batch = jnp.array(batch)\n",
    "#         print(jnp.linalg.norm(batch, axis=1))\n",
    "        batch = jnp.divide(batch, jnp.expand_dims(jnp.linalg.norm(batch, axis=1), axis=1) + 1e-5)\n",
    "#         print(jnp.linalg.norm(batch, axis=1))\n",
    "        train_state = train_step(train_state, batch)\n",
    "        train_state = compute_metrics(state=train_state, batch=batch)\n",
    "        metrics = train_state.metrics.compute()['loss']\n",
    "        ep_loss.append(metrics)\n",
    "    \n",
    "    print(f'Epoch: {ep} Iter {i} Metrics: {jnp.mean(np.array(ep_loss))}')\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def get_HR_jax(state, train_loader, test_loader, top_n=10):\n",
    "\n",
    "    counter, hits = 0, 0\n",
    "    r = []\n",
    "    for i, (batch_train, batch_val) in enumerate(zip(iter(train_loader), iter(test_loader))):\n",
    "        batch_train, batch_val = jnp.array(batch_train), jnp.array(batch_val)\n",
    "        delta = batch_val - batch_train\n",
    "        x_rec = state.apply_fn({'params': state.params}, batch_train)\n",
    "        \n",
    "        for k in range(delta.shape[0]):\n",
    "            non_zero_ind = np.nonzero(np.array(delta[k]))[0]\n",
    "            if len(non_zero_ind) > 0:\n",
    "                counter += 1\n",
    "                diff = set([int(item) for item  in np.argpartition(np.array(x_rec[k]).reshape(-1), -top_n)[-top_n:]]) - (set([int(item) for item in non_zero_ind]))\n",
    "                \n",
    "                if len(diff) < top_n:\n",
    "                    hits += 1\n",
    "    return hits / counter\n",
    "\n",
    "def get_recall_jax(state, train_loader, test_loader, top_n=50):\n",
    "\n",
    "    counter, hits = 0, 0\n",
    "    r = []\n",
    "    for i, (batch_train, batch_val) in enumerate(zip(iter(train_loader), iter(test_loader))):\n",
    "        batch_train, batch_val = jnp.array(batch_train), jnp.array(batch_val)\n",
    "        delta = batch_val - batch_train\n",
    "        \n",
    "        x_rec = state.apply_fn({'params': state.params}, batch_train)\n",
    "        \n",
    "        for k in range(delta.shape[0]):\n",
    "            non_zero_ind = np.nonzero(np.array(delta[k]))[0]\n",
    "            if len(non_zero_ind) > 0:\n",
    "                counter += 1\n",
    "                diff = set([int(item) for item in  np.argpartition(np.array(x_rec[k]).reshape(-1), -top_n)[-top_n:]]).intersection(set([int(item) for item in non_zero_ind]))\n",
    "                r.append(len(diff) / len(non_zero_ind))\n",
    "                if len(diff) < top_n:\n",
    "                    hits += 1\n",
    "    return np.mean(r)\n",
    "        \n",
    "get_recall_jax(train_state, train_dl, test_dl)\n",
    "\n",
    "print(f'R@50: {get_recall_jax(train_state, train_dl, test_dl)} HR@10: {get_HR_jax(train_state, train_dl, test_dl, top_n=10)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
