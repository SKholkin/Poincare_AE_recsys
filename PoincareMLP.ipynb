{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f5036d8e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/skholkin/projects/python_venv/lib/python3.10/site-packages/flax/core/frozen_dict.py:169: FutureWarning: jax.tree_util.register_keypaths is deprecated, and will be removed in a future release. Please use `register_pytree_with_keys()` instead.\n",
      "  jax.tree_util.register_keypaths(\n"
     ]
    }
   ],
   "source": [
    "import jax\n",
    "import jax.numpy as jnp\n",
    "from flax import linen as nn\n",
    "from flax import struct\n",
    "\n",
    "import optax\n",
    "\n",
    "import numpy as np\n",
    "from flax import struct \n",
    "from clu import metrics\n",
    "from dataclasses import field\n",
    "from functools import partial\n",
    "from typing import Any, Callable, Optional, Tuple, Union\n",
    "from flax import struct  \n",
    "\n",
    "from clu import metrics\n",
    "\n",
    "from chex import Array\n",
    "from flax import linen as nn\n",
    "from flax.linen.activation import sigmoid, tanh\n",
    "from flax.linen.dtypes import promote_dtype\n",
    "from flax.linen.initializers import orthogonal\n",
    "from flax.linen.linear import default_kernel_init\n",
    "from jax import numpy as jnp\n",
    "from jax import random, vmap\n",
    "from chex import Array\n",
    "from jax import lax\n",
    "from jax.nn.initializers import Initializer as Initializer\n",
    "from jax._src import dtypes\n",
    "\n",
    "from flax.training import train_state\n",
    "\n",
    "from rieoptax.geometry.hyperbolic import PoincareBall\n",
    "\n",
    "\n",
    "@struct.dataclass\n",
    "class Metrics(metrics.Collection):\n",
    "  loss: metrics.Average.from_output('loss')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "010a5556",
   "metadata": {},
   "source": [
    "### Learning target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c4942ba1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Array(0.9999899, dtype=float32)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# from distributions import NormalPoincareBall\n",
    "\n",
    "\n",
    "key = jax.random.PRNGKey(0)\n",
    "import numpy as np\n",
    "\n",
    "class NormalPoincareBall:\n",
    "    def __init__(self, dim, c):\n",
    "        self.dim = dim\n",
    "        self.c = c\n",
    "        self.ball = PoincareBall(dim, c)\n",
    "        self.base_point = jnp.zeros(dim)\n",
    "    \n",
    "    def sample(self):\n",
    "        seed = np.random.randint(1000)\n",
    "        key = jax.random.PRNGKey(seed)\n",
    "        key, subkey = jax.random.split(key)\n",
    "        euclidian_sample = jax.random.normal(key, shape=self.dim)\n",
    "        poincare_ball_sample = self.ball.exp(self.base_point, euclidian_sample)\n",
    "        return poincare_ball_sample\n",
    "\n",
    "\n",
    "def nonlin(x):\n",
    "    return 4 * x ** 3 +  x ** 2 + 2 * x\n",
    "\n",
    "class NonLinFnPoincareBall:\n",
    "    def __init__(self, in_dim=10, out_dim=5, c=-1):\n",
    "        self.c = c\n",
    "        self.in_dim, self.out_dim = in_dim, out_dim\n",
    "        global key\n",
    "        key, subkey = jax.random.split(key)\n",
    "        self.theta_vec = jax.random.normal(key, shape=(out_dim, in_dim))\n",
    "        \n",
    "        poincare_ball_normal_bias = NormalPoincareBall(dim=(out_dim,), c=-1)\n",
    "        self.theta_bias = poincare_ball_normal_bias.sample()\n",
    "\n",
    "    def apply_fn(self, x):\n",
    "        ball_in = PoincareBall(self.in_dim, self.c)\n",
    "        Ax = ball_in.mobius_matvec(self.theta_vec, x)\n",
    "        ball_out = PoincareBall(self.out_dim, self.c)\n",
    "        Ax_plus_b = ball_out.mobius_add(Ax, self.theta_bias)\n",
    "        \n",
    "        output = ball_out.log(jnp.zeros_like(Ax_plus_b), Ax_plus_b)\n",
    "        output = nonlin(output)\n",
    "        output = ball_out.exp(jnp.zeros_like(output), output)\n",
    "        return output\n",
    "    \n",
    "real_fn_poincare = NonLinFnPoincareBall(in_dim=10, out_dim=5)\n",
    "\n",
    "poincare_ball_normal_x = NormalPoincareBall(dim=(10,), c=-1)\n",
    "\n",
    "sampled_poincare = poincare_ball_normal_x.sample()\n",
    "sampled_y = real_fn_poincare.apply_fn(sampled_poincare)\n",
    "\n",
    "jnp.linalg.norm(sampled_y)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bbad298",
   "metadata": {},
   "source": [
    "### Poincare Linear"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "8b266ecf",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "in_dim, out_dim = 10, 5\n",
    "c = -1\n",
    "\n",
    "class PoincareLinear(nn.Module):\n",
    "    \n",
    "    param_dtype = jnp.float32\n",
    "    kernel_init: Callable = default_kernel_init\n",
    "    bias_init: Callable = nn.initializers.uniform(scale= 1 / jnp.sqrt(in_dim))\n",
    "        \n",
    "    in_dim, out_dim = in_dim, out_dim\n",
    "    c = c\n",
    "    \n",
    "    def setup(self):\n",
    "        self.scalars = self.param(\n",
    "            \"scalars\",\n",
    "            self.kernel_init,\n",
    "            (self.out_dim, self.in_dim),\n",
    "            self.param_dtype,\n",
    "        )\n",
    "    \n",
    "        self.bias_poincare = self.param(\n",
    "            \"bias_poincare\",\n",
    "            self.bias_init,\n",
    "            (self.out_dim,),\n",
    "            self.param_dtype,\n",
    "        )\n",
    "\n",
    "    def __call__(self, x):\n",
    "        ball_in = PoincareBall(self.in_dim, self.c)\n",
    "        y = ball_in.mobius_matvec(self.scalars, x)\n",
    "        \n",
    "        ball_out = PoincareBall(self.out_dim, self.c)\n",
    "\n",
    "        return ball_out.mobius_add(y, self.bias_poincare)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "636451b2",
   "metadata": {},
   "source": [
    "### Training procedure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "38753572",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 1\n",
    "\n",
    "import flax\n",
    "import numpy as np\n",
    "import optax\n",
    "    \n",
    "from flax.training import train_state\n",
    "class TrainStateRiemannian(train_state.TrainState):\n",
    "    metrics: Metrics\n",
    "    \n",
    "    def apply_gradients(self, *, grads, **kwargs):\n",
    "        \"\"\"Updates `step`, `params`, `opt_state` and `**kwargs` in return value.\n",
    "\n",
    "        Note that internally this function calls `.tx.update()` followed by a call\n",
    "        to `optax.apply_updates()` to update `params` and `opt_state`.\n",
    "\n",
    "        Args:\n",
    "          grads: Gradients that have the same pytree structure as `.params`.\n",
    "          **kwargs: Additional dataclass attributes that should be `.replace()`-ed.\n",
    "\n",
    "        Returns:\n",
    "          An updated instance of `self` with `step` incremented by one, `params`\n",
    "          and `opt_state` updated by applying `grads`, and additional attributes\n",
    "          replaced as specified by `kwargs`.\n",
    "        \"\"\"\n",
    "        updates, new_opt_state = self.tx.update(\n",
    "            grads, self.opt_state, self.params)\n",
    "        \n",
    "        \n",
    "        old_bias_params = self.params['bias_poincare']\n",
    "        \n",
    "        ball_bias = PoincareBall(out_dim, c)\n",
    "        \n",
    "        bias_poincare_grads = updates['bias_poincare']\n",
    "        \n",
    "        r_grad_poincare_bias = ball_bias.egrad_to_rgrad(old_bias_params, bias_poincare_grads)\n",
    "        \n",
    "        updates = updates.unfreeze()\n",
    "        updates['bias_poincare'] = r_grad_poincare_bias\n",
    "        updates = flax.core.frozen_dict.freeze(updates)\n",
    "        \n",
    "        new_params = optax.apply_updates(self.params, updates)\n",
    "        \n",
    "        bias_poincare_new = new_params['bias_poincare']\n",
    "        new_params = new_params.unfreeze()\n",
    "        lr = self.lr\n",
    "        tv = lr * r_grad_poincare_bias\n",
    "        \n",
    "        new_params['bias_poincare'] = ball_bias.exp(old_bias_params, tv)\n",
    "        \n",
    "        new_params = flax.core.frozen_dict.freeze(new_params)\n",
    "        \n",
    "        return self.replace(\n",
    "            step=self.step + 1,\n",
    "            params=new_params,\n",
    "            opt_state=new_opt_state,\n",
    "            **kwargs,\n",
    "        )\n",
    "\n",
    "\n",
    "\n",
    "def create_train_state(module, rng, learning_rate):\n",
    "  \"\"\"Creates an initial `TrainState`.\"\"\"\n",
    "  from flax.training import train_state\n",
    "  class TrainState(TrainStateRiemannian):\n",
    "    metrics: Metrics\n",
    "    lr = learning_rate\n",
    "  params = module.init(rng, jnp.ones([in_dim]))['params'] # initialize parameters by passing a template image\n",
    "  tx = optax.sgd(learning_rate)\n",
    "  return TrainState.create(\n",
    "      apply_fn=module.apply, params=params, tx=tx,\n",
    "      metrics=Metrics.empty())\n",
    "\n",
    "\n",
    "# @jax.jit\n",
    "def train_step(state, batch_x, batch_y):\n",
    "  \"\"\"Train for a single step.\"\"\"\n",
    "\n",
    "  def loss_fn(params):\n",
    "    y = state.apply_fn({'params': params}, batch_x)\n",
    "    loss = ((y - batch_y) ** 2).sum()\n",
    "    return loss\n",
    "  \n",
    "  grad_fn = jax.grad(loss_fn)\n",
    "  grads = grad_fn(state.params)\n",
    "  state = state.apply_gradients(grads=grads)\n",
    "  return state\n",
    "\n",
    "@jax.jit\n",
    "def compute_metrics(*, state, batch_x, batch_y):\n",
    "    y = state.apply_fn({'params': state.params}, batch_x)\n",
    "    loss = ((y - batch_y) ** 2).mean()\n",
    "    metric_updates = state.metrics.single_from_model_output(loss=loss)\n",
    "    metrics = state.metrics.merge(metric_updates)\n",
    "    state = state.replace(metrics=metrics)\n",
    "    return state\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9a40da09",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'PoincareLinear' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m poincare_linear \u001b[38;5;241m=\u001b[39m \u001b[43mPoincareLinear\u001b[49m()\n\u001b[1;32m      2\u001b[0m train_state \u001b[38;5;241m=\u001b[39m create_train_state(poincare_linear, jax\u001b[38;5;241m.\u001b[39mrandom\u001b[38;5;241m.\u001b[39mPRNGKey(\u001b[38;5;241m43\u001b[39m), \u001b[38;5;241m1e-02\u001b[39m)\n\u001b[1;32m      4\u001b[0m n_iter \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m10000\u001b[39m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'PoincareLinear' is not defined"
     ]
    }
   ],
   "source": [
    "\n",
    "poincare_linear = PoincareLinear()\n",
    "train_state = create_train_state(poincare_linear, jax.random.PRNGKey(43), 1e-02)\n",
    "\n",
    "n_iter = 10000\n",
    "\n",
    "for i in range(n_iter):\n",
    "    \n",
    "    batch_x = poincare_ball_normal_x.sample()\n",
    "    batch_y = real_fn_poincare.apply_fn(batch_x)\n",
    "    \n",
    "    train_state = train_step(train_state, batch_x, batch_y)\n",
    "    train_state = compute_metrics(state=train_state, batch_x=batch_x,batch_y=batch_y)\n",
    "    \n",
    "    loss = train_state.metrics.compute()['loss']\n",
    "    if i % 100 == 0:\n",
    "        print(f'{i} Loss: {loss}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6df2c32e",
   "metadata": {},
   "source": [
    "### Poincare MLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a6809d3f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "in_dim, out_dim = 10, 5\n",
    "\n",
    "hidden_dim = 100\n",
    "\n",
    "dims = [in_dim, hidden_dim, out_dim]\n",
    "c = -1\n",
    "\n",
    "class PoincareMLP(nn.Module):\n",
    "    \n",
    "    param_dtype = jnp.float32\n",
    "    kernel_init: Callable = default_kernel_init\n",
    "    bias_init: Callable = nn.initializers.uniform(scale= 1 / jnp.sqrt(in_dim))\n",
    "        \n",
    "    in_dim, out_dim = in_dim, out_dim\n",
    "    hidden_dim = hidden_dim\n",
    "    c = c\n",
    "    act = nn.sigmoid\n",
    "    \n",
    "    def setup(self):\n",
    "        self.scalars_1 = self.param(\n",
    "            \"scalars@1\",\n",
    "            self.kernel_init,\n",
    "            (self.hidden_dim, self.in_dim),\n",
    "            self.param_dtype,\n",
    "        )\n",
    "    \n",
    "        self.bias_poincare_1 = self.param(\n",
    "            \"bias_poincare@1\",\n",
    "            self.bias_init,\n",
    "            (self.hidden_dim,),\n",
    "            self.param_dtype,\n",
    "        )\n",
    "        \n",
    "        self.scalars_2 = self.param(\n",
    "            \"scalars@2\",\n",
    "            self.kernel_init,\n",
    "            (self.out_dim, self.hidden_dim),\n",
    "            self.param_dtype,\n",
    "        )\n",
    "    \n",
    "        self.bias_poincare_2 = self.param(\n",
    "            \"bias_poincare@2\",\n",
    "            self.bias_init,\n",
    "            (self.out_dim,),\n",
    "            self.param_dtype,\n",
    "        )\n",
    "        \n",
    "        self.balls = {'bias_poincare@1': PoincareBall(self.hidden_dim, self.c),\n",
    "                      'bias_poincare@2': PoincareBall(self.out_dim, self.c)}\n",
    "        \n",
    "\n",
    "    def __call__(self, x):\n",
    "        # Linear 1\n",
    "        ball_in = PoincareBall(self.in_dim, self.c)\n",
    "        Ax = ball_in.mobius_matvec(self.scalars_1, x)\n",
    "        ball_hidden = PoincareBall(self.hidden_dim, self.c)\n",
    "        Ax_b = ball_hidden.mobius_add(Ax, self.bias_poincare_1)\n",
    "        \n",
    "        # Activation\n",
    "        activation_hid = ball_hidden.log(jnp.zeros_like(Ax_b), Ax_b)\n",
    "        activation_hid = nonlin(activation_hid)\n",
    "        activation_hid = ball_hidden.exp(jnp.zeros_like(activation_hid), activation_hid)\n",
    "        \n",
    "        # Linear 2\n",
    "        ball_out = PoincareBall(self.out_dim, self.c)\n",
    "        output = ball_in.mobius_matvec(self.scalars_2, activation_hid)\n",
    "        output = ball_out.mobius_add(output, self.bias_poincare_2)\n",
    "        return output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "dfff6e45",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from flax.training import train_state\n",
    "class TrainStateRiemannianMLP(train_state.TrainState):\n",
    "    metrics: Metrics\n",
    "    \n",
    "    balls = {'bias_poincare@1': PoincareBall(hidden_dim, c),\n",
    "                  'bias_poincare@2': PoincareBall(out_dim, c)}\n",
    "    \n",
    "    def apply_gradients(self, *, grads, **kwargs):\n",
    "        \"\"\"Updates `step`, `params`, `opt_state` and `**kwargs` in return value.\n",
    "\n",
    "        Note that internally this function calls `.tx.update()` followed by a call\n",
    "        to `optax.apply_updates()` to update `params` and `opt_state`.\n",
    "\n",
    "        Args:\n",
    "          grads: Gradients that have the same pytree structure as `.params`.\n",
    "          **kwargs: Additional dataclass attributes that should be `.replace()`-ed.\n",
    "\n",
    "        Returns:\n",
    "          An updated instance of `self` with `step` incremented by one, `params`\n",
    "          and `opt_state` updated by applying `grads`, and additional attributes\n",
    "          replaced as specified by `kwargs`.\n",
    "        \"\"\"\n",
    "        updates, new_opt_state = self.tx.update(\n",
    "            grads, self.opt_state, self.params)\n",
    "        \n",
    "        \n",
    "        poincare_param_names = self.balls.keys()\n",
    "        r_grad_poincare_bias, old_bias_params = {}, {}\n",
    "        for name in poincare_param_names:\n",
    "            \n",
    "            old_bias_params[name] = self.params[name]\n",
    "#             print('old_bias_params', old_bias_params.shape)\n",
    "#             ball_bias = PoincareBall(out_dim, c)\n",
    "            bias_poincare_grads = updates[name]\n",
    "            r_grad_poincare_bias[name] = self.balls[name].egrad_to_rgrad(old_bias_params[name], bias_poincare_grads)\n",
    "        \n",
    "        updates = updates.unfreeze()\n",
    "        for name in poincare_param_names:\n",
    "            updates[name] = r_grad_poincare_bias[name]\n",
    "            \n",
    "        updates = flax.core.frozen_dict.freeze(updates)\n",
    "        \n",
    "        new_params = optax.apply_updates(self.params, updates)\n",
    "        \n",
    "        lr = self.lr\n",
    "        new_params = new_params.unfreeze()\n",
    "        for name in poincare_param_names:\n",
    "            bias_poincare_new = new_params[name]\n",
    "            tv = lr * r_grad_poincare_bias[name]\n",
    "            new_params[name] = self.balls[name].exp(old_bias_params[name], tv)\n",
    "        \n",
    "        new_params = flax.core.frozen_dict.freeze(new_params)\n",
    "        \n",
    "        return self.replace(\n",
    "            step=self.step + 1,\n",
    "            params=new_params,\n",
    "            opt_state=new_opt_state,\n",
    "            **kwargs,\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "39b0eba6",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_safe_grads_coef = 100\n",
    "@jax.jit\n",
    "def train_step(state, batch_x, batch_y):\n",
    "  \"\"\"Train for a single step.\"\"\"\n",
    "\n",
    "  def loss_fn(params):\n",
    "    y = state.apply_fn({'params': params}, batch_x)\n",
    "    loss = 1000 * ((y - batch_y) ** 2).sum()\n",
    "    return loss\n",
    "  \n",
    "  grad_fn = jax.grad(loss_fn)\n",
    "  grads = grad_fn(state.params)\n",
    "  state = state.apply_gradients(grads=grads)\n",
    "  return state\n",
    "\n",
    "def create_train_state(module, rng, learning_rate):\n",
    "  \"\"\"Creates an initial `TrainState`.\"\"\"\n",
    "  from flax.training import train_state\n",
    "  class TrainState(TrainStateRiemannianMLP):\n",
    "    metrics: Metrics\n",
    "    lr = learning_rate\n",
    "  params = module.init(rng, jnp.ones([in_dim]))['params'] # initialize parameters by passing a template image\n",
    "  tx = optax.sgd(learning_rate)\n",
    "  return TrainState.create(\n",
    "      apply_fn=module.apply, params=params, tx=tx,\n",
    "      metrics=Metrics.empty())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "2f6f3e13",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 Loss: 0.05742352083325386\n",
      "100 Loss: 0.11213408410549164\n",
      "200 Loss: 0.07124282419681549\n",
      "300 Loss: 0.05154486745595932\n",
      "400 Loss: 0.040676213800907135\n",
      "500 Loss: 0.03365926817059517\n",
      "600 Loss: 0.028710979968309402\n",
      "700 Loss: 0.025007419288158417\n",
      "800 Loss: 0.022199496626853943\n",
      "900 Loss: 0.020025629550218582\n",
      "1000 Loss: 0.018245309591293335\n",
      "1100 Loss: 0.016704751178622246\n",
      "1200 Loss: 0.01546729076653719\n",
      "1300 Loss: 0.014344530180096626\n",
      "1400 Loss: 0.013374610804021358\n",
      "1500 Loss: 0.012529105879366398\n",
      "1600 Loss: 0.011789258569478989\n",
      "1700 Loss: 0.011152444407343864\n",
      "1800 Loss: 0.010583269409835339\n",
      "1900 Loss: 0.010052936151623726\n",
      "2000 Loss: 0.009694541804492474\n",
      "2100 Loss: 0.009276802651584148\n",
      "2200 Loss: 0.008880427107214928\n",
      "2300 Loss: 0.008538922294974327\n",
      "2400 Loss: 0.00823428574949503\n",
      "2500 Loss: 0.007926433347165585\n",
      "2600 Loss: 0.007645555306226015\n",
      "2700 Loss: 0.007375412620604038\n",
      "2800 Loss: 0.007122365292161703\n",
      "2900 Loss: 0.006889529526233673\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[37], line 9\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(n_iter):\n\u001b[1;32m      8\u001b[0m     batch_x \u001b[38;5;241m=\u001b[39m poincare_ball_normal_x\u001b[38;5;241m.\u001b[39msample()\n\u001b[0;32m----> 9\u001b[0m     batch_y \u001b[38;5;241m=\u001b[39m \u001b[43mreal_fn_poincare\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch_x\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     11\u001b[0m     train_state \u001b[38;5;241m=\u001b[39m train_step(train_state, batch_x, batch_y)\n\u001b[1;32m     12\u001b[0m     train_state \u001b[38;5;241m=\u001b[39m compute_metrics(state\u001b[38;5;241m=\u001b[39mtrain_state, batch_x\u001b[38;5;241m=\u001b[39mbatch_x,batch_y\u001b[38;5;241m=\u001b[39mbatch_y)\n",
      "Cell \u001b[0;32mIn[8], line 44\u001b[0m, in \u001b[0;36mNonLinFnPoincareBall.apply_fn\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     41\u001b[0m Ax_plus_b \u001b[38;5;241m=\u001b[39m ball_out\u001b[38;5;241m.\u001b[39mmobius_add(Ax, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtheta_bias)\n\u001b[1;32m     43\u001b[0m output \u001b[38;5;241m=\u001b[39m ball_out\u001b[38;5;241m.\u001b[39mlog(jnp\u001b[38;5;241m.\u001b[39mzeros_like(Ax_plus_b), Ax_plus_b)\n\u001b[0;32m---> 44\u001b[0m output \u001b[38;5;241m=\u001b[39m \u001b[43mnonlin\u001b[49m\u001b[43m(\u001b[49m\u001b[43moutput\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     45\u001b[0m output \u001b[38;5;241m=\u001b[39m ball_out\u001b[38;5;241m.\u001b[39mexp(jnp\u001b[38;5;241m.\u001b[39mzeros_like(output), output)\n\u001b[1;32m     46\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m output\n",
      "Cell \u001b[0;32mIn[8], line 24\u001b[0m, in \u001b[0;36mnonlin\u001b[0;34m(x)\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mnonlin\u001b[39m(x):\n\u001b[0;32m---> 24\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;241m4\u001b[39m \u001b[38;5;241m*\u001b[39m x \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m \u001b[38;5;241m3\u001b[39m \u001b[38;5;241m+\u001b[39m  \u001b[43mx\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m2\u001b[39;49m \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m2\u001b[39m \u001b[38;5;241m*\u001b[39m x\n",
      "File \u001b[0;32m~/projects/python_venv/lib/python3.10/site-packages/jax/_src/numpy/lax_numpy.py:5112\u001b[0m, in \u001b[0;36m_defer_to_unrecognized_arg.<locals>.deferring_binary_op\u001b[0;34m(self, other)\u001b[0m\n\u001b[1;32m   5110\u001b[0m args \u001b[38;5;241m=\u001b[39m (other, \u001b[38;5;28mself\u001b[39m) \u001b[38;5;28;01mif\u001b[39;00m swap \u001b[38;5;28;01melse\u001b[39;00m (\u001b[38;5;28mself\u001b[39m, other)\n\u001b[1;32m   5111\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(other, _accepted_binop_types):\n\u001b[0;32m-> 5112\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mbinary_op\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   5113\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(other, _rejected_binop_types):\n\u001b[1;32m   5114\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124munsupported operand type(s) for \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mopchar\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m: \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   5115\u001b[0m                   \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(args[\u001b[38;5;241m0\u001b[39m])\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m!r}\u001b[39;00m\u001b[38;5;124m and \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(args[\u001b[38;5;241m1\u001b[39m])\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m!r}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/projects/python_venv/lib/python3.10/site-packages/jax/_src/numpy/ufuncs.py:342\u001b[0m, in \u001b[0;36mpower\u001b[0;34m(x1, x2)\u001b[0m\n\u001b[1;32m    340\u001b[0m     \u001b[38;5;28;01mpass\u001b[39;00m\n\u001b[1;32m    341\u001b[0m   \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 342\u001b[0m     x1, \u001b[38;5;241m=\u001b[39m \u001b[43m_promote_dtypes_numeric\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx1\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    343\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m lax\u001b[38;5;241m.\u001b[39minteger_pow(x1, x2)\n\u001b[1;32m    344\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m _power(x1, x2)\n",
      "File \u001b[0;32m~/projects/python_venv/lib/python3.10/site-packages/jax/_src/numpy/util.py:302\u001b[0m, in \u001b[0;36m_promote_dtypes_numeric\u001b[0;34m(*args)\u001b[0m\n\u001b[1;32m    298\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_promote_dtypes_numeric\u001b[39m(\u001b[38;5;241m*\u001b[39margs: ArrayLike) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m List[Array]:\n\u001b[1;32m    299\u001b[0m \u001b[38;5;250m  \u001b[39m\u001b[38;5;124;03m\"\"\"Convenience function to apply Numpy argument dtype promotion.\u001b[39;00m\n\u001b[1;32m    300\u001b[0m \n\u001b[1;32m    301\u001b[0m \u001b[38;5;124;03m  Promotes arguments to a numeric (non-bool) type.\"\"\"\u001b[39;00m\n\u001b[0;32m--> 302\u001b[0m   to_dtype, weak_type \u001b[38;5;241m=\u001b[39m \u001b[43mdtypes\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_lattice_result_type\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    303\u001b[0m   to_dtype \u001b[38;5;241m=\u001b[39m dtypes\u001b[38;5;241m.\u001b[39mcanonicalize_dtype(to_dtype)\n\u001b[1;32m    304\u001b[0m   to_dtype_numeric \u001b[38;5;241m=\u001b[39m dtypes\u001b[38;5;241m.\u001b[39mto_numeric_dtype(to_dtype)\n",
      "File \u001b[0;32m~/projects/python_venv/lib/python3.10/site-packages/jax/_src/dtypes.py:642\u001b[0m, in \u001b[0;36m_lattice_result_type\u001b[0;34m(*args)\u001b[0m\n\u001b[1;32m    640\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_lattice_result_type\u001b[39m(\u001b[38;5;241m*\u001b[39margs: Any) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tuple[DType, \u001b[38;5;28mbool\u001b[39m]:\n\u001b[1;32m    641\u001b[0m   dtypes, weak_types \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mzip\u001b[39m(\u001b[38;5;241m*\u001b[39m(_dtype_and_weaktype(arg) \u001b[38;5;28;01mfor\u001b[39;00m arg \u001b[38;5;129;01min\u001b[39;00m args))\n\u001b[0;32m--> 642\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mdtypes\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m    643\u001b[0m     out_dtype \u001b[38;5;241m=\u001b[39m dtypes[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m    644\u001b[0m     out_weak_type \u001b[38;5;241m=\u001b[39m weak_types[\u001b[38;5;241m0\u001b[39m]\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "\n",
    "poincare_linear = PoincareMLP()\n",
    "train_state = create_train_state(poincare_linear, jax.random.PRNGKey(42), 1e-03)\n",
    "\n",
    "n_iter = 10000\n",
    "\n",
    "for i in range(n_iter):\n",
    "    \n",
    "    batch_x = poincare_ball_normal_x.sample()\n",
    "    batch_y = real_fn_poincare.apply_fn(batch_x)\n",
    "    \n",
    "    train_state = train_step(train_state, batch_x, batch_y)\n",
    "    train_state = compute_metrics(state=train_state, batch_x=batch_x,batch_y=batch_y)\n",
    "    \n",
    "    loss = train_state.metrics.compute()['loss']\n",
    "    if i % 100 == 0:\n",
    "        print(f'{i} Loss: {loss}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00eea307",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4ea7747",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
