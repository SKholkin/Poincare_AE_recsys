{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/skholkin/projects/python_venv/lib/python3.10/site-packages/flax/core/frozen_dict.py:169: FutureWarning: jax.tree_util.register_keypaths is deprecated, and will be removed in a future release. Please use `register_pytree_with_keys()` instead.\n",
      "  jax.tree_util.register_keypaths(\n"
     ]
    }
   ],
   "source": [
    "import jax\n",
    "import jax.numpy as jnp\n",
    "from flax import linen as nn\n",
    "from flax import struct\n",
    "\n",
    "import optax\n",
    "\n",
    "import numpy as np\n",
    "from flax import struct \n",
    "from clu import metrics\n",
    "from dataclasses import field\n",
    "from functools import partial\n",
    "from typing import Any, Callable, Optional, Tuple, Union\n",
    "from flax import struct  \n",
    "\n",
    "from clu import metrics\n",
    "\n",
    "from chex import Array\n",
    "from flax import linen as nn\n",
    "from flax.linen.activation import sigmoid, tanh\n",
    "from flax.linen.dtypes import promote_dtype\n",
    "from flax.linen.initializers import orthogonal\n",
    "from flax.linen.linear import default_kernel_init\n",
    "from jax import numpy as jnp\n",
    "from jax import random, vmap\n",
    "from chex import Array\n",
    "from jax import lax\n",
    "from jax.nn.initializers import Initializer as Initializer\n",
    "from jax._src import dtypes\n",
    "\n",
    "from flax.training import train_state\n",
    "\n",
    "from rieoptax.geometry.hyperbolic import PoincareBall\n",
    "\n",
    "\n",
    "@struct.dataclass\n",
    "class Metrics(metrics.Collection):\n",
    "  loss: metrics.Average.from_output('loss')\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Learning target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Array(0.9999899, dtype=float32)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "key = jax.random.PRNGKey(0)\n",
    "import numpy as np\n",
    "\n",
    "class NormalPoincareBall:\n",
    "    def __init__(self, dim, c):\n",
    "        self.dim = dim\n",
    "        self.c = c\n",
    "        self.ball = PoincareBall(dim, c)\n",
    "        self.base_point = jnp.zeros(dim)\n",
    "    \n",
    "    def sample(self):\n",
    "        seed = np.random.randint(1000)\n",
    "        key = jax.random.PRNGKey(seed)\n",
    "        key, subkey = jax.random.split(key)\n",
    "        euclidian_sample = jax.random.normal(key, shape=self.dim)\n",
    "        poincare_ball_sample = self.ball.exp(self.base_point, euclidian_sample)\n",
    "        return poincare_ball_sample\n",
    "\n",
    "\n",
    "def nonlin(x):\n",
    "    return 4 * x ** 3 +  x ** 2 + 2 * x\n",
    "\n",
    "class NonLinFnPoincareBall:\n",
    "    def __init__(self, in_dim=10, out_dim=5, c=-1):\n",
    "        self.c = c\n",
    "        self.in_dim, self.out_dim = in_dim, out_dim\n",
    "        global key\n",
    "        key, subkey = jax.random.split(key)\n",
    "        self.theta_vec = jax.random.normal(key, shape=(out_dim, in_dim))\n",
    "        \n",
    "        poincare_ball_normal_bias = NormalPoincareBall(dim=(out_dim,), c=-1)\n",
    "        self.theta_bias = poincare_ball_normal_bias.sample()\n",
    "\n",
    "    def apply_fn(self, x):\n",
    "        ball_in = PoincareBall(self.in_dim, self.c)\n",
    "        Ax = ball_in.mobius_matvec(self.theta_vec, x)\n",
    "        ball_out = PoincareBall(self.out_dim, self.c)\n",
    "        Ax_plus_b = ball_out.mobius_add(Ax, self.theta_bias)\n",
    "        \n",
    "        output = ball_out.log(jnp.zeros_like(Ax_plus_b), Ax_plus_b)\n",
    "        output = nonlin(output)\n",
    "        output = ball_out.exp(jnp.zeros_like(output), output)\n",
    "        return output\n",
    "    \n",
    "real_fn_poincare = NonLinFnPoincareBall(in_dim=100, out_dim=100)\n",
    "\n",
    "poincare_ball_normal_x = NormalPoincareBall(dim=(100,), c=-1)\n",
    "\n",
    "sampled_poincare = poincare_ball_normal_x.sample()\n",
    "sampled_y = real_fn_poincare.apply_fn(sampled_poincare)\n",
    "\n",
    "jnp.linalg.norm(sampled_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 1\n",
    "\n",
    "import flax\n",
    "import numpy as np\n",
    "import optax\n",
    "    \n",
    "from flax.training import train_state\n",
    "class TrainStateRiemannianMLP(train_state.TrainState):\n",
    "    metrics: Metrics\n",
    "    \n",
    "    balls = {'bias_poincare@1': PoincareBall(hidden_dim, c),\n",
    "                  'bias_poincare@2': PoincareBall(out_dim, c)}\n",
    "    \n",
    "    def apply_gradients(self, *, grads, **kwargs):\n",
    "        \"\"\"Updates `step`, `params`, `opt_state` and `**kwargs` in return value.\n",
    "\n",
    "        Note that internally this function calls `.tx.update()` followed by a call\n",
    "        to `optax.apply_updates()` to update `params` and `opt_state`.\n",
    "\n",
    "        Args:\n",
    "          grads: Gradients that have the same pytree structure as `.params`.\n",
    "          **kwargs: Additional dataclass attributes that should be `.replace()`-ed.\n",
    "\n",
    "        Returns:\n",
    "          An updated instance of `self` with `step` incremented by one, `params`\n",
    "          and `opt_state` updated by applying `grads`, and additional attributes\n",
    "          replaced as specified by `kwargs`.\n",
    "        \"\"\"\n",
    "        updates, new_opt_state = self.tx.update(\n",
    "            grads, self.opt_state, self.params)\n",
    "        \n",
    "        \n",
    "        poincare_param_names = self.balls.keys()\n",
    "        r_grad_poincare_bias, old_bias_params = {}, {}\n",
    "        for name in poincare_param_names:\n",
    "            \n",
    "            old_bias_params[name] = self.params[name]\n",
    "#             print('old_bias_params', old_bias_params.shape)\n",
    "#             ball_bias = PoincareBall(out_dim, c)\n",
    "            bias_poincare_grads = updates[name]\n",
    "            r_grad_poincare_bias[name] = self.balls[name].egrad_to_rgrad(old_bias_params[name], bias_poincare_grads)\n",
    "        \n",
    "        updates = updates.unfreeze()\n",
    "        for name in poincare_param_names:\n",
    "            updates[name] = r_grad_poincare_bias[name]\n",
    "            \n",
    "        updates = flax.core.frozen_dict.freeze(updates)\n",
    "        \n",
    "        new_params = optax.apply_updates(self.params, updates)\n",
    "        \n",
    "        lr = self.lr\n",
    "        new_params = new_params.unfreeze()\n",
    "        for name in poincare_param_names:\n",
    "            bias_poincare_new = new_params[name]\n",
    "            tv = lr * r_grad_poincare_bias[name]\n",
    "            new_params[name] = self.balls[name].exp(old_bias_params[name], tv)\n",
    "        \n",
    "        new_params = flax.core.frozen_dict.freeze(new_params)\n",
    "        \n",
    "        return self.replace(\n",
    "            step=self.step + 1,\n",
    "            params=new_params,\n",
    "            opt_state=new_opt_state,\n",
    "            **kwargs,\n",
    "        )\n",
    "\n",
    "\n",
    "def create_train_state(module, rng, learning_rate):\n",
    "  \"\"\"Creates an initial `TrainState`.\"\"\"\n",
    "  from flax.training import train_state\n",
    "  class TrainState(TrainStateRiemannianMLP):\n",
    "    metrics: Metrics\n",
    "    lr = learning_rate\n",
    "  params = module.init(rng, jnp.ones([in_dim]))['params'] # initialize parameters by passing a template image\n",
    "  tx = optax.sgd(learning_rate)\n",
    "  return TrainState.create(\n",
    "      apply_fn=module.apply, params=params, tx=tx,\n",
    "      metrics=Metrics.empty())\n",
    "\n",
    "\n",
    "@jax.jit\n",
    "def train_step(state, batch_x, batch_y):\n",
    "  \"\"\"Train for a single step.\"\"\"\n",
    "\n",
    "  def loss_fn(params):\n",
    "    y = state.apply_fn({'params': params}, batch_x)\n",
    "    loss = ((y - batch_y) ** 2).sum()\n",
    "    return loss\n",
    "  \n",
    "  grad_fn = jax.grad(loss_fn)\n",
    "  grads = grad_fn(state.params)\n",
    "  state = state.apply_gradients(grads=grads)\n",
    "  return state\n",
    "\n",
    "@jax.jit\n",
    "def compute_metrics(*, state, batch_x, batch_y):\n",
    "    y = state.apply_fn({'params': state.params}, batch_x)\n",
    "    loss = ((y - batch_y) ** 2).mean()\n",
    "    metric_updates = state.metrics.single_from_model_output(loss=loss)\n",
    "    metrics = state.metrics.merge(metric_updates)\n",
    "    state = state.replace(metrics=metrics)\n",
    "    return state\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Poincare AutoEncoder One Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "in_dim, out_dim = 100, 100\n",
    "\n",
    "hidden_dim = 10\n",
    "\n",
    "dims = [in_dim, hidden_dim, out_dim]\n",
    "c = -1\n",
    "\n",
    "class PoincareSimpleAE(nn.Module):\n",
    "    \n",
    "    param_dtype = jnp.float32\n",
    "    kernel_init: Callable = default_kernel_init\n",
    "    bias_init: Callable = nn.initializers.uniform(scale= 1 / jnp.sqrt(in_dim))\n",
    "        \n",
    "    in_dim, out_dim = in_dim, out_dim\n",
    "    hidden_dim = hidden_dim\n",
    "    c = c\n",
    "    act = nn.sigmoid\n",
    "    \n",
    "    def setup(self):\n",
    "        self.scalars_1 = self.param(\n",
    "            \"scalars@1\",\n",
    "            self.kernel_init,\n",
    "            (self.hidden_dim, self.in_dim),\n",
    "            self.param_dtype,\n",
    "        )\n",
    "    \n",
    "        self.bias_poincare_1 = self.param(\n",
    "            \"bias_poincare@1\",\n",
    "            self.bias_init,\n",
    "            (self.hidden_dim,),\n",
    "            self.param_dtype,\n",
    "        )\n",
    "        \n",
    "        self.scalars_2 = self.param(\n",
    "            \"scalars@2\",\n",
    "            self.kernel_init,\n",
    "            (self.out_dim, self.hidden_dim),\n",
    "            self.param_dtype,\n",
    "        )\n",
    "    \n",
    "        self.bias_poincare_2 = self.param(\n",
    "            \"bias_poincare@2\",\n",
    "            self.bias_init,\n",
    "            (self.out_dim,),\n",
    "            self.param_dtype,\n",
    "        )\n",
    "        \n",
    "        self.balls = {'bias_poincare@1': PoincareBall(self.hidden_dim, self.c),\n",
    "                      'bias_poincare@2': PoincareBall(self.out_dim, self.c)}\n",
    "        \n",
    "\n",
    "    def __call__(self, x):\n",
    "        # Linear 1\n",
    "        ball_in = PoincareBall(self.in_dim, self.c)\n",
    "        Ax = ball_in.mobius_matvec(self.scalars_1, x)\n",
    "        ball_hidden = PoincareBall(self.hidden_dim, self.c)\n",
    "        Ax_b = ball_hidden.mobius_add(Ax, self.bias_poincare_1)\n",
    "        \n",
    "        # Activation\n",
    "        activation_hid = ball_hidden.log(jnp.zeros_like(Ax_b), Ax_b)\n",
    "        activation_hid = nonlin(activation_hid)\n",
    "        activation_hid = ball_hidden.exp(jnp.zeros_like(activation_hid), activation_hid)\n",
    "        \n",
    "        # Linear 2\n",
    "        ball_out = PoincareBall(self.out_dim, self.c)\n",
    "        output = ball_in.mobius_matvec(self.scalars_2, activation_hid)\n",
    "        output = ball_out.mobius_add(output, self.bias_poincare_2)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 Loss: 0.021675221621990204\n",
      "100 Loss: 0.01950104720890522\n",
      "200 Loss: 0.019348137080669403\n",
      "300 Loss: 0.018962200731039047\n",
      "400 Loss: 0.018658241257071495\n",
      "500 Loss: 0.018314179033041\n",
      "600 Loss: 0.018054114654660225\n",
      "700 Loss: 0.017775366082787514\n",
      "800 Loss: 0.017533093690872192\n",
      "900 Loss: 0.01728835515677929\n",
      "1000 Loss: 0.017055699601769447\n",
      "1100 Loss: 0.016848359256982803\n",
      "1200 Loss: 0.01664067804813385\n",
      "1300 Loss: 0.01644635945558548\n",
      "1400 Loss: 0.016289275139570236\n",
      "1500 Loss: 0.01610618643462658\n",
      "1600 Loss: 0.01595291495323181\n",
      "1700 Loss: 0.015813441947102547\n",
      "1800 Loss: 0.0157018955796957\n",
      "1900 Loss: 0.01557418517768383\n",
      "2000 Loss: 0.015450805425643921\n",
      "2100 Loss: 0.01534909475594759\n",
      "2200 Loss: 0.015241052024066448\n",
      "2300 Loss: 0.01514815166592598\n",
      "2400 Loss: 0.01504920981824398\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[24], line 9\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(n_iter):\n\u001b[1;32m      8\u001b[0m     batch_x \u001b[38;5;241m=\u001b[39m poincare_ball_normal_x\u001b[38;5;241m.\u001b[39msample()\n\u001b[0;32m----> 9\u001b[0m     batch_y \u001b[38;5;241m=\u001b[39m \u001b[43mreal_fn_poincare\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch_x\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     11\u001b[0m     train_state \u001b[38;5;241m=\u001b[39m train_step(train_state, batch_y, batch_y)\n\u001b[1;32m     12\u001b[0m     train_state \u001b[38;5;241m=\u001b[39m compute_metrics(state\u001b[38;5;241m=\u001b[39mtrain_state, batch_x\u001b[38;5;241m=\u001b[39mbatch_y, batch_y\u001b[38;5;241m=\u001b[39mbatch_y)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "\n",
    "poincare_ae = PoincareSimpleAE()\n",
    "train_state = create_train_state(poincare_ae, jax.random.PRNGKey(42), 1e-02)\n",
    "\n",
    "n_iter = 10000\n",
    "\n",
    "for i in range(n_iter):\n",
    "    \n",
    "    batch_x = poincare_ball_normal_x.sample()\n",
    "    batch_y = real_fn_poincare.apply_fn(batch_x)\n",
    "    \n",
    "    train_state = train_step(train_state, batch_y, batch_y)\n",
    "    train_state = compute_metrics(state=train_state, batch_x=batch_y, batch_y=batch_y)\n",
    "    \n",
    "    loss = train_state.metrics.compute()['loss']\n",
    "    if i % 100 == 0:\n",
    "        print(f'{i} Loss: {loss}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
